<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>ise.models.training.Trainer API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ise.models.training.Trainer</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import numpy as np
np.random.seed(10)
import torch
from torch import optim, nn
from ise.models.training.dataclasses import PyTorchDataset, TSDataset
from torch.utils.data import DataLoader
import time
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime

device = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;)


class Trainer:
    def __init__(self, ):
        self.model = None
        self.data = {}
        self.device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)  # Determine whether on GPU or not
        self.num_input_features = None
        self.loss_logs = {&#39;training_loss&#39;: [], &#39;vasqrtl_loss&#39;: [], }
        self.train_loader = None
        self.test_loader = None
        self.data_dict = None
        self.logs = {&#39;training&#39;: {&#39;epoch&#39;: [], &#39;batch&#39;: []}, &#39;testing&#39;: []}

    def _format_data(self, train_features, train_labels, test_features, test_labels, train_batch_size=100,
                     test_batch_size=10, sequence_length=5):
        &#34;&#34;&#34;Takes training and testing dataframes and converts them into PyTorch DataLoaders to be used in the training loop.

        Args:
            train_features (pd.DataFrame|np.array): training dataset features
            train_labels (pd.Series|np.array): training dataset labels
            test_features (pd.DataFrame|np.array): test dataset features
            test_labels (pd.Series|np.array): test dataset labels
            train_batch_size (int, optional): batch size for training loop. Defaults to 100.
            test_batch_size (int, optional): batch size for validation loop. Defaults to 10.

        Returns:
            self (Trainer): Trainer object
        &#34;&#34;&#34;

        # Convert to Numpy first (no direct conversion from pd.DataFrame to torch.tensor)
        self.X_train = np.array(train_features, dtype=np.float64)
        self.y_train = np.array(train_labels, dtype=np.float64)
        self.X_test = np.array(test_features, dtype=np.float64)
        self.y_test = np.array(test_labels, dtype=np.float64)

        if self.time_series:
            train_dataset = TSDataset(
                torch.from_numpy(self.X_train).float(),
                torch.from_numpy(self.y_train).float().squeeze(),
                sequence_length=sequence_length,
            )
            test_dataset = TSDataset(
                torch.from_numpy(self.X_test).float(),
                torch.from_numpy(self.y_test).float().squeeze(),
                sequence_length=sequence_length,
            )
        else:
            train_dataset = PyTorchDataset(torch.from_numpy(self.X_train).float(),
                                           torch.from_numpy(self.y_train).float().squeeze())
            test_dataset = PyTorchDataset(torch.from_numpy(self.X_test).float(),
                                          torch.from_numpy(self.y_test).float().squeeze())

        # Create dataset and data loaders to be used in training loop
        self.train_loader = DataLoader(dataset=train_dataset, batch_size=train_batch_size, shuffle=True)
        self.test_loader = DataLoader(dataset=test_dataset, batch_size=test_batch_size, )

        return self

    def _initiate_model(self, model_class, data_dict, architecture, sequence_length, batch_size, mc_dropout, dropout_prob):
        # save attributes
        self.data_dict = data_dict
        self.num_input_features = self.data_dict[&#39;train_features&#39;].shape[1]
        
        # TODO: Write load_saved_model method in model file
        # Loop through possible architecture parameters and if it not given, set it to None
        for param in [&#39;num_linear_layers&#39;, &#39;nodes&#39;, &#39;num_rnn_hidden&#39;, &#39;num_rnn_layers&#39;]:
            try:
                architecture[param]
            except:
                architecture[param] = None
        architecture[&#39;input_layer_size&#39;] = self.num_input_features

        # establish model - if using exploratory model, use num_linear_layers and nodes arg
        self.model = model_class(architecture=architecture, mc_dropout=mc_dropout, dropout_prob=dropout_prob).to(self.device)
        self.time_series = True if hasattr(self.model, &#39;time_series&#39;) else False

        # If the data loader hasn&#39;t been created, run _format_data function
        if self.train_loader is None or self.train_loader is None:
            self._format_data(data_dict[&#39;train_features&#39;], data_dict[&#39;train_labels&#39;], data_dict[&#39;test_features&#39;],
                              data_dict[&#39;test_labels&#39;],
                              train_batch_size=batch_size,
                              sequence_length=sequence_length,
                              )

    def train(self, model_class, data_dict, criterion, epochs, batch_size, mc_dropout=False, dropout_prob=None, tensorboard=False, architecture=None,
              save_model=False, performance_optimized=False, verbose=True, sequence_length=5, tensorboard_comment=None):
        &#34;&#34;&#34;Training loop for training a PyTorch model. Include validation, GPU compatibility, and tensorboard integration.

        Args:
            model_class (ModelClass): Model to be trained. Usually custom model class.
            data_dict (dict): Dictionary containing training and testing arrays/tensors.
                    Example: {&#39;train_features&#39;: train_features, train_labels&#39;: train_labels, &#39;test_features&#39;: test_features, &#39;test_labels&#39;: test_labels,}
            criterion (torch.nn.Loss): Loss class from PyTorch NN module.
                    Example: torch.nn.MSELoss()
            epochs (int): Number of training epochs
            batch_size (int): Number of training batches
            tensorboard (bool, optional): Flag determining whether Tensorboard logs should be generated and outputted. Defaults to False.
            num_linear_layers (int, optional): Number of linear layers in the model. Only used if paired with ExploratoryModel. Defaults to None.
            nodes (list, optional): List of integers denoting the number of nodes in num_linear_layers. Len(nodes) must equal num_linear_layers. Defaults to None.
            save_model (bool, optional): Flag determining whether the trained model should be saved. Defaults to False.
            performance_optimized (bool, optional): Flag determining whether the training loop should be optimized for fast training. Defaults to False.
        &#34;&#34;&#34;
        
        # Initiates model with inputted architecture and formats data
        self._initiate_model(model_class, data_dict, architecture, sequence_length, batch_size, mc_dropout, dropout_prob)

        # Use multiple GPU parallelization if available
        # if torch.cuda.device_count() &gt; 1:
        #     self.model = nn.DataParallel(self.model)

        optimizer = optim.Adam(self.model.parameters(), )
        # criterion = nn.MSELoss()
        self.time = datetime.now().strftime(r&#34;%d-%m-%Y %H.%M.%S&#34;)

        # tensorboard_comment = f&#34; -- {self.time}, FC={architecture[&#39;num_linear_layers&#39;]}, nodes={architecture[&#39;nodes&#39;]}, batch_size={batch_size},&#34;
        # comment = f&#34; -- {self.time}, dataset={dataset},&#34;
        if tensorboard:
            tb = SummaryWriter(comment=tensorboard_comment)
        mae = nn.L1Loss()
        X_test = torch.tensor(self.X_test, dtype=torch.float).to(self.device)
        y_test = torch.tensor(self.y_test, dtype=torch.float).to(self.device)
        for epoch in range(1, epochs + 1):
            self.model.train()
            epoch_start = time.time()

            total_loss = 0
            total_mae = 0
            for X_train_batch, y_train_batch in self.train_loader:
                X_train_batch, y_train_batch = X_train_batch.to(self.device), y_train_batch.to(self.device)

                optimizer.zero_grad()

                pred = self.model(X_train_batch)
                loss = criterion(pred, y_train_batch.unsqueeze(1))
                loss.backward()
                optimizer.step()

                total_loss += loss.item()
                self.logs[&#39;training&#39;][&#39;batch&#39;].append(loss.item())

                if not performance_optimized:
                    total_mae += mae(pred, y_train_batch.unsqueeze(1)).item()

            avg_mse = total_loss / len(self.train_loader)
            self.logs[&#39;training&#39;][&#39;epoch&#39;].append(avg_mse)

            if not performance_optimized:
                avg_rmse = np.sqrt(avg_mse)
                avg_mae = total_mae / len(self.train_loader)

            training_end = time.time()

            if not performance_optimized:
                self.model.eval()
                test_total_loss = 0
                test_total_mae = 0
                for X_test_batch, y_test_batch in self.test_loader:
                    X_test_batch, y_test_batch = X_test_batch.to(self.device), y_test_batch.to(self.device)
                    test_pred = self.model(X_test_batch)
                    loss = criterion(test_pred, y_test_batch.unsqueeze(1))
                    test_total_loss += loss.item()
                    test_total_mae += mae(test_pred, y_test_batch.unsqueeze(1)).item()

                test_mse = test_total_loss / len(self.test_loader)
                test_mae = test_total_mae / len(self.test_loader)
                self.logs[&#39;testing&#39;].append(test_mse)
                testing_end = time.time()

                preds, _, _, _, _ = self.model.predict(self.X_test, mc_iterations=1)
                if self.device.type != &#39;cuda&#39;:
                    r2 = r2_score(self.y_test, preds)
                else:
                    r2 = r2_score(self.y_test, preds)

            if tensorboard:
                tb.add_scalar(&#34;Training MSE&#34;, avg_mse, epoch)

                if not performance_optimized:
                    tb.add_scalar(&#34;Training RMSE&#34;, avg_rmse, epoch)
                    tb.add_scalar(&#34;Training MAE&#34;, avg_mae, epoch)

                    tb.add_scalar(&#34;Validation MSE&#34;, test_mse, epoch)
                    tb.add_scalar(&#34;Validation RMSE&#34;, np.sqrt(test_mse), epoch)
                    tb.add_scalar(&#34;Validation MAE&#34;, test_mae, epoch)
                    tb.add_scalar(&#34;R^2&#34;, r2, epoch)

            if verbose:
                if not performance_optimized:
                    print(&#39;&#39;)
                    print(f&#34;&#34;&#34;Epoch: {epoch}/{epochs}, Training Loss (MSE): {avg_mse:0.8f}, Validation Loss (MSE): {test_mse:0.8f}
    Training time: {training_end - epoch_start: 0.2f} seconds, Validation time: {testing_end - training_end: 0.2f} seconds&#34;&#34;&#34;)

                else:
                    print(&#39;&#39;)
                    print(f&#34;&#34;&#34;Epoch: {epoch}/{epochs}, Training Loss (MSE): {avg_mse:0.8f}
    Training time: {training_end - epoch_start: 0.2f} seconds&#34;&#34;&#34;)

        if tensorboard:
            metrics, _ = self.evaluate()
            tb.add_hparams(
                {&#34;rnn_layers&#34;: architecture[&#39;num_rnn_layers&#39;], &#34;hidden&#34;: architecture[&#39;num_rnn_hidden&#39;], &#34;batch_size&#34;: batch_size, },

                {
                    &#34;Test MSE&#34;: metrics[&#39;MSE&#39;], &#34;Test MAE&#34;: metrics[&#39;MAE&#39;], &#34;R^2&#34;: metrics[&#39;R2&#39;],
                    &#34;RMSE&#34;: metrics[&#34;RMSE&#34;]
                },
            )

            tb.close()

        if save_model:
            if isinstance(save_model, str):
                model_path = f&#34;{save_model}/{self.time}.pt&#34;
                
            elif isinstance(save_model, bool):
                import os
                dirname = os.path.dirname(__file__)
                model_path = os.path.join(dirname, f&#34;../{self.time}.pt&#34;)
            
            torch.save(self.model.state_dict(), model_path)
            print(&#39;&#39;)
            print(f&#39;Model saved to {model_path}&#39;)
            print(&#39;&#39;)

        return self

    def plot_loss(self, save=False):
        plt.plot(self.logs[&#39;training&#39;][&#39;epoch&#39;], &#39;r-&#39;, label=&#39;Training Loss&#39;)
        plt.plot(self.logs[&#39;testing&#39;], &#39;b-&#39;, label=&#39;Validation Loss&#39;)
        plt.title(&#39;Emulator MSE loss per Epoch&#39;)
        plt.xlabel(f&#39;Epoch #&#39;)
        plt.ylabel(&#39;Loss (MSE)&#39;)
        plt.legend()

        if save:
            plt.savefig(save)
        plt.show()

    def evaluate(self, verbose=True):
        # Test predictions
        self.model.eval()
        preds = torch.tensor([]).to(self.device)
        for X_test_batch, y_test_batch in self.test_loader:
            X_test_batch, y_test_batch = X_test_batch.to(self.device), y_test_batch.to(self.device)
            test_pred = self.model(X_test_batch)
            preds = torch.cat((preds, test_pred), 0)

        if self.device.type == &#39;cuda&#39;:
            preds = preds.squeeze().cpu().detach().numpy()
        else:
            preds = preds.squeeze().detach().numpy()
            
        mse = sum((preds - self.y_test.squeeze())**2) / len(preds)
        mae = sum(abs((preds - self.y_test.squeeze()))) / len(preds)
        rmse = np.sqrt(mse)
        r2 = r2_score(self.y_test, preds)

        metrics = {&#39;MSE&#39;: mse, &#39;MAE&#39;: mae, &#39;RMSE&#39;: rmse, &#39;R2&#39;: r2}

        if verbose:
            print(f&#34;&#34;&#34;Test Metrics
MSE: {mse:0.6f}
MAE: {mae:0.6f}
RMSE: {rmse:0.6f}
R2: {r2:0.6f}&#34;&#34;&#34;)

        return metrics, preds</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ise.models.training.Trainer.Trainer"><code class="flex name class">
<span>class <span class="ident">Trainer</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Trainer:
    def __init__(self, ):
        self.model = None
        self.data = {}
        self.device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)  # Determine whether on GPU or not
        self.num_input_features = None
        self.loss_logs = {&#39;training_loss&#39;: [], &#39;vasqrtl_loss&#39;: [], }
        self.train_loader = None
        self.test_loader = None
        self.data_dict = None
        self.logs = {&#39;training&#39;: {&#39;epoch&#39;: [], &#39;batch&#39;: []}, &#39;testing&#39;: []}

    def _format_data(self, train_features, train_labels, test_features, test_labels, train_batch_size=100,
                     test_batch_size=10, sequence_length=5):
        &#34;&#34;&#34;Takes training and testing dataframes and converts them into PyTorch DataLoaders to be used in the training loop.

        Args:
            train_features (pd.DataFrame|np.array): training dataset features
            train_labels (pd.Series|np.array): training dataset labels
            test_features (pd.DataFrame|np.array): test dataset features
            test_labels (pd.Series|np.array): test dataset labels
            train_batch_size (int, optional): batch size for training loop. Defaults to 100.
            test_batch_size (int, optional): batch size for validation loop. Defaults to 10.

        Returns:
            self (Trainer): Trainer object
        &#34;&#34;&#34;

        # Convert to Numpy first (no direct conversion from pd.DataFrame to torch.tensor)
        self.X_train = np.array(train_features, dtype=np.float64)
        self.y_train = np.array(train_labels, dtype=np.float64)
        self.X_test = np.array(test_features, dtype=np.float64)
        self.y_test = np.array(test_labels, dtype=np.float64)

        if self.time_series:
            train_dataset = TSDataset(
                torch.from_numpy(self.X_train).float(),
                torch.from_numpy(self.y_train).float().squeeze(),
                sequence_length=sequence_length,
            )
            test_dataset = TSDataset(
                torch.from_numpy(self.X_test).float(),
                torch.from_numpy(self.y_test).float().squeeze(),
                sequence_length=sequence_length,
            )
        else:
            train_dataset = PyTorchDataset(torch.from_numpy(self.X_train).float(),
                                           torch.from_numpy(self.y_train).float().squeeze())
            test_dataset = PyTorchDataset(torch.from_numpy(self.X_test).float(),
                                          torch.from_numpy(self.y_test).float().squeeze())

        # Create dataset and data loaders to be used in training loop
        self.train_loader = DataLoader(dataset=train_dataset, batch_size=train_batch_size, shuffle=True)
        self.test_loader = DataLoader(dataset=test_dataset, batch_size=test_batch_size, )

        return self

    def _initiate_model(self, model_class, data_dict, architecture, sequence_length, batch_size, mc_dropout, dropout_prob):
        # save attributes
        self.data_dict = data_dict
        self.num_input_features = self.data_dict[&#39;train_features&#39;].shape[1]
        
        # TODO: Write load_saved_model method in model file
        # Loop through possible architecture parameters and if it not given, set it to None
        for param in [&#39;num_linear_layers&#39;, &#39;nodes&#39;, &#39;num_rnn_hidden&#39;, &#39;num_rnn_layers&#39;]:
            try:
                architecture[param]
            except:
                architecture[param] = None
        architecture[&#39;input_layer_size&#39;] = self.num_input_features

        # establish model - if using exploratory model, use num_linear_layers and nodes arg
        self.model = model_class(architecture=architecture, mc_dropout=mc_dropout, dropout_prob=dropout_prob).to(self.device)
        self.time_series = True if hasattr(self.model, &#39;time_series&#39;) else False

        # If the data loader hasn&#39;t been created, run _format_data function
        if self.train_loader is None or self.train_loader is None:
            self._format_data(data_dict[&#39;train_features&#39;], data_dict[&#39;train_labels&#39;], data_dict[&#39;test_features&#39;],
                              data_dict[&#39;test_labels&#39;],
                              train_batch_size=batch_size,
                              sequence_length=sequence_length,
                              )

    def train(self, model_class, data_dict, criterion, epochs, batch_size, mc_dropout=False, dropout_prob=None, tensorboard=False, architecture=None,
              save_model=False, performance_optimized=False, verbose=True, sequence_length=5, tensorboard_comment=None):
        &#34;&#34;&#34;Training loop for training a PyTorch model. Include validation, GPU compatibility, and tensorboard integration.

        Args:
            model_class (ModelClass): Model to be trained. Usually custom model class.
            data_dict (dict): Dictionary containing training and testing arrays/tensors.
                    Example: {&#39;train_features&#39;: train_features, train_labels&#39;: train_labels, &#39;test_features&#39;: test_features, &#39;test_labels&#39;: test_labels,}
            criterion (torch.nn.Loss): Loss class from PyTorch NN module.
                    Example: torch.nn.MSELoss()
            epochs (int): Number of training epochs
            batch_size (int): Number of training batches
            tensorboard (bool, optional): Flag determining whether Tensorboard logs should be generated and outputted. Defaults to False.
            num_linear_layers (int, optional): Number of linear layers in the model. Only used if paired with ExploratoryModel. Defaults to None.
            nodes (list, optional): List of integers denoting the number of nodes in num_linear_layers. Len(nodes) must equal num_linear_layers. Defaults to None.
            save_model (bool, optional): Flag determining whether the trained model should be saved. Defaults to False.
            performance_optimized (bool, optional): Flag determining whether the training loop should be optimized for fast training. Defaults to False.
        &#34;&#34;&#34;
        
        # Initiates model with inputted architecture and formats data
        self._initiate_model(model_class, data_dict, architecture, sequence_length, batch_size, mc_dropout, dropout_prob)

        # Use multiple GPU parallelization if available
        # if torch.cuda.device_count() &gt; 1:
        #     self.model = nn.DataParallel(self.model)

        optimizer = optim.Adam(self.model.parameters(), )
        # criterion = nn.MSELoss()
        self.time = datetime.now().strftime(r&#34;%d-%m-%Y %H.%M.%S&#34;)

        # tensorboard_comment = f&#34; -- {self.time}, FC={architecture[&#39;num_linear_layers&#39;]}, nodes={architecture[&#39;nodes&#39;]}, batch_size={batch_size},&#34;
        # comment = f&#34; -- {self.time}, dataset={dataset},&#34;
        if tensorboard:
            tb = SummaryWriter(comment=tensorboard_comment)
        mae = nn.L1Loss()
        X_test = torch.tensor(self.X_test, dtype=torch.float).to(self.device)
        y_test = torch.tensor(self.y_test, dtype=torch.float).to(self.device)
        for epoch in range(1, epochs + 1):
            self.model.train()
            epoch_start = time.time()

            total_loss = 0
            total_mae = 0
            for X_train_batch, y_train_batch in self.train_loader:
                X_train_batch, y_train_batch = X_train_batch.to(self.device), y_train_batch.to(self.device)

                optimizer.zero_grad()

                pred = self.model(X_train_batch)
                loss = criterion(pred, y_train_batch.unsqueeze(1))
                loss.backward()
                optimizer.step()

                total_loss += loss.item()
                self.logs[&#39;training&#39;][&#39;batch&#39;].append(loss.item())

                if not performance_optimized:
                    total_mae += mae(pred, y_train_batch.unsqueeze(1)).item()

            avg_mse = total_loss / len(self.train_loader)
            self.logs[&#39;training&#39;][&#39;epoch&#39;].append(avg_mse)

            if not performance_optimized:
                avg_rmse = np.sqrt(avg_mse)
                avg_mae = total_mae / len(self.train_loader)

            training_end = time.time()

            if not performance_optimized:
                self.model.eval()
                test_total_loss = 0
                test_total_mae = 0
                for X_test_batch, y_test_batch in self.test_loader:
                    X_test_batch, y_test_batch = X_test_batch.to(self.device), y_test_batch.to(self.device)
                    test_pred = self.model(X_test_batch)
                    loss = criterion(test_pred, y_test_batch.unsqueeze(1))
                    test_total_loss += loss.item()
                    test_total_mae += mae(test_pred, y_test_batch.unsqueeze(1)).item()

                test_mse = test_total_loss / len(self.test_loader)
                test_mae = test_total_mae / len(self.test_loader)
                self.logs[&#39;testing&#39;].append(test_mse)
                testing_end = time.time()

                preds, _, _, _, _ = self.model.predict(self.X_test, mc_iterations=1)
                if self.device.type != &#39;cuda&#39;:
                    r2 = r2_score(self.y_test, preds)
                else:
                    r2 = r2_score(self.y_test, preds)

            if tensorboard:
                tb.add_scalar(&#34;Training MSE&#34;, avg_mse, epoch)

                if not performance_optimized:
                    tb.add_scalar(&#34;Training RMSE&#34;, avg_rmse, epoch)
                    tb.add_scalar(&#34;Training MAE&#34;, avg_mae, epoch)

                    tb.add_scalar(&#34;Validation MSE&#34;, test_mse, epoch)
                    tb.add_scalar(&#34;Validation RMSE&#34;, np.sqrt(test_mse), epoch)
                    tb.add_scalar(&#34;Validation MAE&#34;, test_mae, epoch)
                    tb.add_scalar(&#34;R^2&#34;, r2, epoch)

            if verbose:
                if not performance_optimized:
                    print(&#39;&#39;)
                    print(f&#34;&#34;&#34;Epoch: {epoch}/{epochs}, Training Loss (MSE): {avg_mse:0.8f}, Validation Loss (MSE): {test_mse:0.8f}
    Training time: {training_end - epoch_start: 0.2f} seconds, Validation time: {testing_end - training_end: 0.2f} seconds&#34;&#34;&#34;)

                else:
                    print(&#39;&#39;)
                    print(f&#34;&#34;&#34;Epoch: {epoch}/{epochs}, Training Loss (MSE): {avg_mse:0.8f}
    Training time: {training_end - epoch_start: 0.2f} seconds&#34;&#34;&#34;)

        if tensorboard:
            metrics, _ = self.evaluate()
            tb.add_hparams(
                {&#34;rnn_layers&#34;: architecture[&#39;num_rnn_layers&#39;], &#34;hidden&#34;: architecture[&#39;num_rnn_hidden&#39;], &#34;batch_size&#34;: batch_size, },

                {
                    &#34;Test MSE&#34;: metrics[&#39;MSE&#39;], &#34;Test MAE&#34;: metrics[&#39;MAE&#39;], &#34;R^2&#34;: metrics[&#39;R2&#39;],
                    &#34;RMSE&#34;: metrics[&#34;RMSE&#34;]
                },
            )

            tb.close()

        if save_model:
            if isinstance(save_model, str):
                model_path = f&#34;{save_model}/{self.time}.pt&#34;
                
            elif isinstance(save_model, bool):
                import os
                dirname = os.path.dirname(__file__)
                model_path = os.path.join(dirname, f&#34;../{self.time}.pt&#34;)
            
            torch.save(self.model.state_dict(), model_path)
            print(&#39;&#39;)
            print(f&#39;Model saved to {model_path}&#39;)
            print(&#39;&#39;)

        return self

    def plot_loss(self, save=False):
        plt.plot(self.logs[&#39;training&#39;][&#39;epoch&#39;], &#39;r-&#39;, label=&#39;Training Loss&#39;)
        plt.plot(self.logs[&#39;testing&#39;], &#39;b-&#39;, label=&#39;Validation Loss&#39;)
        plt.title(&#39;Emulator MSE loss per Epoch&#39;)
        plt.xlabel(f&#39;Epoch #&#39;)
        plt.ylabel(&#39;Loss (MSE)&#39;)
        plt.legend()

        if save:
            plt.savefig(save)
        plt.show()

    def evaluate(self, verbose=True):
        # Test predictions
        self.model.eval()
        preds = torch.tensor([]).to(self.device)
        for X_test_batch, y_test_batch in self.test_loader:
            X_test_batch, y_test_batch = X_test_batch.to(self.device), y_test_batch.to(self.device)
            test_pred = self.model(X_test_batch)
            preds = torch.cat((preds, test_pred), 0)

        if self.device.type == &#39;cuda&#39;:
            preds = preds.squeeze().cpu().detach().numpy()
        else:
            preds = preds.squeeze().detach().numpy()
            
        mse = sum((preds - self.y_test.squeeze())**2) / len(preds)
        mae = sum(abs((preds - self.y_test.squeeze()))) / len(preds)
        rmse = np.sqrt(mse)
        r2 = r2_score(self.y_test, preds)

        metrics = {&#39;MSE&#39;: mse, &#39;MAE&#39;: mae, &#39;RMSE&#39;: rmse, &#39;R2&#39;: r2}

        if verbose:
            print(f&#34;&#34;&#34;Test Metrics
MSE: {mse:0.6f}
MAE: {mae:0.6f}
RMSE: {rmse:0.6f}
R2: {r2:0.6f}&#34;&#34;&#34;)

        return metrics, preds</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="ise.models.training.Trainer.Trainer.evaluate"><code class="name flex">
<span>def <span class="ident">evaluate</span></span>(<span>self, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">    def evaluate(self, verbose=True):
        # Test predictions
        self.model.eval()
        preds = torch.tensor([]).to(self.device)
        for X_test_batch, y_test_batch in self.test_loader:
            X_test_batch, y_test_batch = X_test_batch.to(self.device), y_test_batch.to(self.device)
            test_pred = self.model(X_test_batch)
            preds = torch.cat((preds, test_pred), 0)

        if self.device.type == &#39;cuda&#39;:
            preds = preds.squeeze().cpu().detach().numpy()
        else:
            preds = preds.squeeze().detach().numpy()
            
        mse = sum((preds - self.y_test.squeeze())**2) / len(preds)
        mae = sum(abs((preds - self.y_test.squeeze()))) / len(preds)
        rmse = np.sqrt(mse)
        r2 = r2_score(self.y_test, preds)

        metrics = {&#39;MSE&#39;: mse, &#39;MAE&#39;: mae, &#39;RMSE&#39;: rmse, &#39;R2&#39;: r2}

        if verbose:
            print(f&#34;&#34;&#34;Test Metrics
MSE: {mse:0.6f}
MAE: {mae:0.6f}
RMSE: {rmse:0.6f}
R2: {r2:0.6f}&#34;&#34;&#34;)

        return metrics, preds</code></pre>
</details>
</dd>
<dt id="ise.models.training.Trainer.Trainer.plot_loss"><code class="name flex">
<span>def <span class="ident">plot_loss</span></span>(<span>self, save=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_loss(self, save=False):
    plt.plot(self.logs[&#39;training&#39;][&#39;epoch&#39;], &#39;r-&#39;, label=&#39;Training Loss&#39;)
    plt.plot(self.logs[&#39;testing&#39;], &#39;b-&#39;, label=&#39;Validation Loss&#39;)
    plt.title(&#39;Emulator MSE loss per Epoch&#39;)
    plt.xlabel(f&#39;Epoch #&#39;)
    plt.ylabel(&#39;Loss (MSE)&#39;)
    plt.legend()

    if save:
        plt.savefig(save)
    plt.show()</code></pre>
</details>
</dd>
<dt id="ise.models.training.Trainer.Trainer.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, model_class, data_dict, criterion, epochs, batch_size, mc_dropout=False, dropout_prob=None, tensorboard=False, architecture=None, save_model=False, performance_optimized=False, verbose=True, sequence_length=5, tensorboard_comment=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Training loop for training a PyTorch model. Include validation, GPU compatibility, and tensorboard integration.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model_class</code></strong> :&ensp;<code>ModelClass</code></dt>
<dd>Model to be trained. Usually custom model class.</dd>
<dt><strong><code>data_dict</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary containing training and testing arrays/tensors.
Example: {'train_features': train_features, train_labels': train_labels, 'test_features': test_features, 'test_labels': test_labels,}</dd>
<dt><strong><code>criterion</code></strong> :&ensp;<code>torch.nn.Loss</code></dt>
<dd>Loss class from PyTorch NN module.
Example: torch.nn.MSELoss()</dd>
<dt><strong><code>epochs</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of training epochs</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of training batches</dd>
<dt><strong><code>tensorboard</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Flag determining whether Tensorboard logs should be generated and outputted. Defaults to False.</dd>
<dt><strong><code>num_linear_layers</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of linear layers in the model. Only used if paired with ExploratoryModel. Defaults to None.</dd>
<dt><strong><code>nodes</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>List of integers denoting the number of nodes in num_linear_layers. Len(nodes) must equal num_linear_layers. Defaults to None.</dd>
<dt><strong><code>save_model</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Flag determining whether the trained model should be saved. Defaults to False.</dd>
<dt><strong><code>performance_optimized</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Flag determining whether the training loop should be optimized for fast training. Defaults to False.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self, model_class, data_dict, criterion, epochs, batch_size, mc_dropout=False, dropout_prob=None, tensorboard=False, architecture=None,
          save_model=False, performance_optimized=False, verbose=True, sequence_length=5, tensorboard_comment=None):
    &#34;&#34;&#34;Training loop for training a PyTorch model. Include validation, GPU compatibility, and tensorboard integration.

    Args:
        model_class (ModelClass): Model to be trained. Usually custom model class.
        data_dict (dict): Dictionary containing training and testing arrays/tensors.
                Example: {&#39;train_features&#39;: train_features, train_labels&#39;: train_labels, &#39;test_features&#39;: test_features, &#39;test_labels&#39;: test_labels,}
        criterion (torch.nn.Loss): Loss class from PyTorch NN module.
                Example: torch.nn.MSELoss()
        epochs (int): Number of training epochs
        batch_size (int): Number of training batches
        tensorboard (bool, optional): Flag determining whether Tensorboard logs should be generated and outputted. Defaults to False.
        num_linear_layers (int, optional): Number of linear layers in the model. Only used if paired with ExploratoryModel. Defaults to None.
        nodes (list, optional): List of integers denoting the number of nodes in num_linear_layers. Len(nodes) must equal num_linear_layers. Defaults to None.
        save_model (bool, optional): Flag determining whether the trained model should be saved. Defaults to False.
        performance_optimized (bool, optional): Flag determining whether the training loop should be optimized for fast training. Defaults to False.
    &#34;&#34;&#34;
    
    # Initiates model with inputted architecture and formats data
    self._initiate_model(model_class, data_dict, architecture, sequence_length, batch_size, mc_dropout, dropout_prob)

    # Use multiple GPU parallelization if available
    # if torch.cuda.device_count() &gt; 1:
    #     self.model = nn.DataParallel(self.model)

    optimizer = optim.Adam(self.model.parameters(), )
    # criterion = nn.MSELoss()
    self.time = datetime.now().strftime(r&#34;%d-%m-%Y %H.%M.%S&#34;)

    # tensorboard_comment = f&#34; -- {self.time}, FC={architecture[&#39;num_linear_layers&#39;]}, nodes={architecture[&#39;nodes&#39;]}, batch_size={batch_size},&#34;
    # comment = f&#34; -- {self.time}, dataset={dataset},&#34;
    if tensorboard:
        tb = SummaryWriter(comment=tensorboard_comment)
    mae = nn.L1Loss()
    X_test = torch.tensor(self.X_test, dtype=torch.float).to(self.device)
    y_test = torch.tensor(self.y_test, dtype=torch.float).to(self.device)
    for epoch in range(1, epochs + 1):
        self.model.train()
        epoch_start = time.time()

        total_loss = 0
        total_mae = 0
        for X_train_batch, y_train_batch in self.train_loader:
            X_train_batch, y_train_batch = X_train_batch.to(self.device), y_train_batch.to(self.device)

            optimizer.zero_grad()

            pred = self.model(X_train_batch)
            loss = criterion(pred, y_train_batch.unsqueeze(1))
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            self.logs[&#39;training&#39;][&#39;batch&#39;].append(loss.item())

            if not performance_optimized:
                total_mae += mae(pred, y_train_batch.unsqueeze(1)).item()

        avg_mse = total_loss / len(self.train_loader)
        self.logs[&#39;training&#39;][&#39;epoch&#39;].append(avg_mse)

        if not performance_optimized:
            avg_rmse = np.sqrt(avg_mse)
            avg_mae = total_mae / len(self.train_loader)

        training_end = time.time()

        if not performance_optimized:
            self.model.eval()
            test_total_loss = 0
            test_total_mae = 0
            for X_test_batch, y_test_batch in self.test_loader:
                X_test_batch, y_test_batch = X_test_batch.to(self.device), y_test_batch.to(self.device)
                test_pred = self.model(X_test_batch)
                loss = criterion(test_pred, y_test_batch.unsqueeze(1))
                test_total_loss += loss.item()
                test_total_mae += mae(test_pred, y_test_batch.unsqueeze(1)).item()

            test_mse = test_total_loss / len(self.test_loader)
            test_mae = test_total_mae / len(self.test_loader)
            self.logs[&#39;testing&#39;].append(test_mse)
            testing_end = time.time()

            preds, _, _, _, _ = self.model.predict(self.X_test, mc_iterations=1)
            if self.device.type != &#39;cuda&#39;:
                r2 = r2_score(self.y_test, preds)
            else:
                r2 = r2_score(self.y_test, preds)

        if tensorboard:
            tb.add_scalar(&#34;Training MSE&#34;, avg_mse, epoch)

            if not performance_optimized:
                tb.add_scalar(&#34;Training RMSE&#34;, avg_rmse, epoch)
                tb.add_scalar(&#34;Training MAE&#34;, avg_mae, epoch)

                tb.add_scalar(&#34;Validation MSE&#34;, test_mse, epoch)
                tb.add_scalar(&#34;Validation RMSE&#34;, np.sqrt(test_mse), epoch)
                tb.add_scalar(&#34;Validation MAE&#34;, test_mae, epoch)
                tb.add_scalar(&#34;R^2&#34;, r2, epoch)

        if verbose:
            if not performance_optimized:
                print(&#39;&#39;)
                print(f&#34;&#34;&#34;Epoch: {epoch}/{epochs}, Training Loss (MSE): {avg_mse:0.8f}, Validation Loss (MSE): {test_mse:0.8f}
Training time: {training_end - epoch_start: 0.2f} seconds, Validation time: {testing_end - training_end: 0.2f} seconds&#34;&#34;&#34;)

            else:
                print(&#39;&#39;)
                print(f&#34;&#34;&#34;Epoch: {epoch}/{epochs}, Training Loss (MSE): {avg_mse:0.8f}
Training time: {training_end - epoch_start: 0.2f} seconds&#34;&#34;&#34;)

    if tensorboard:
        metrics, _ = self.evaluate()
        tb.add_hparams(
            {&#34;rnn_layers&#34;: architecture[&#39;num_rnn_layers&#39;], &#34;hidden&#34;: architecture[&#39;num_rnn_hidden&#39;], &#34;batch_size&#34;: batch_size, },

            {
                &#34;Test MSE&#34;: metrics[&#39;MSE&#39;], &#34;Test MAE&#34;: metrics[&#39;MAE&#39;], &#34;R^2&#34;: metrics[&#39;R2&#39;],
                &#34;RMSE&#34;: metrics[&#34;RMSE&#34;]
            },
        )

        tb.close()

    if save_model:
        if isinstance(save_model, str):
            model_path = f&#34;{save_model}/{self.time}.pt&#34;
            
        elif isinstance(save_model, bool):
            import os
            dirname = os.path.dirname(__file__)
            model_path = os.path.join(dirname, f&#34;../{self.time}.pt&#34;)
        
        torch.save(self.model.state_dict(), model_path)
        print(&#39;&#39;)
        print(f&#39;Model saved to {model_path}&#39;)
        print(&#39;&#39;)

    return self</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ise.models.training" href="index.html">ise.models.training</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ise.models.training.Trainer.Trainer" href="#ise.models.training.Trainer.Trainer">Trainer</a></code></h4>
<ul class="">
<li><code><a title="ise.models.training.Trainer.Trainer.evaluate" href="#ise.models.training.Trainer.Trainer.evaluate">evaluate</a></code></li>
<li><code><a title="ise.models.training.Trainer.Trainer.plot_loss" href="#ise.models.training.Trainer.Trainer.plot_loss">plot_loss</a></code></li>
<li><code><a title="ise.models.training.Trainer.Trainer.train" href="#ise.models.training.Trainer.Trainer.train">train</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>
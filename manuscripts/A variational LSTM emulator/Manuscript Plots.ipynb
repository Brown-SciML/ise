{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b842fff",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db1b584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from ise.sectors.visualization import Plotter\n",
    "from ise.sectors.models.testing.pretrained import binned_sle_table\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import ScalarFormatter, AutoMinorLocator\n",
    "from ise.sectors.utils.data import (\n",
    "    create_distribution,\n",
    "    kl_divergence,\n",
    "    js_divergence\n",
    ")\n",
    "from itertools import product\n",
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a51b260",
   "metadata": {},
   "source": [
    "## Load data and set MPL params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83d0d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figures = False\n",
    "dataset = pd.read_csv(r\"nn_results.csv\")\n",
    "gp_data = pd.read_csv(r\"gp_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77ed364",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = gp_data['preds']\n",
    "std = gp_data['std']\n",
    "dataset['gp_pred'] = preds\n",
    "dataset['gp_std'] = std\n",
    "dataset['gp_mse'] = abs(dataset.gp_pred - dataset.true)**2\n",
    "dataset['gp_mae'] = abs(dataset.gp_pred - dataset.true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aa6a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['xtick.labelsize'] = 16\n",
    "mpl.rcParams['ytick.labelsize'] = 16\n",
    "mpl.rcParams['font.size'] = 15\n",
    "mpl.rcParams['figure.autolayout'] = True\n",
    "mpl.rcParams['figure.figsize'] = 7.2,4.45\n",
    "# mpl.rcParams['axes.titlesize '] = 16\n",
    "# mpl.rcParams['axes.labelsize '] = 17\n",
    "# mpl.rcParams['lines.linewidth '] = 2\n",
    "# mpl.rcParams['lines.markersize '] = 6\n",
    "mpl.rcParams['legend.fontsize'] = 13\n",
    "mpl.rcParams['mathtext.fontset'] = 'stix'\n",
    "mpl.rcParams['font.family'] = 'STIXGeneral'\n",
    "\n",
    "true_color = '#66c2a5'\n",
    "nn_color = '#fc8d62'\n",
    "gp_color = '#8da0cb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d027a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.style.use('plot_style.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382b4c5a",
   "metadata": {},
   "source": [
    "## Format data by grouping by 85-year projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2983af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_run(dataset: pd.DataFrame, column: str=None, condition: str=None,):\n",
    "    \"\"\"Groups the dataset into each individual simulation series by both the true value of the \n",
    "    simulated SLE as well as the model predicted SLE. The resulting arrays are NXM matrices with\n",
    "    N being the number of simulations and M being 85, or the length of the series.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): Dataset to be grouped\n",
    "        column (str, optional): Column to subset on. Defaults to None.\n",
    "        condition (str, optional): Condition to subset with. Can be int, str, float, etc. Defaults to None.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Tuple containing [all_trues, all_preds], or NXM matrices of each series corresponding to true values and predicted values.\n",
    "    \"\"\"\n",
    "\n",
    "    modelnames = dataset.modelname.unique()\n",
    "    exp_ids = dataset.exp_id.unique()\n",
    "    sectors = dataset.sectors.unique()\n",
    "\n",
    "    all_runs = [list(i) for i in list(product(modelnames, exp_ids, sectors))]\n",
    "\n",
    "    all_trues = []\n",
    "    all_preds = []\n",
    "    scenarios = []\n",
    "    all_gp = []\n",
    "    all_std = []\n",
    "    for i, run in enumerate(all_runs):\n",
    "        modelname = run[0]\n",
    "        exp = run[1]\n",
    "        sector = run[2]\n",
    "        if column is None and condition is None:\n",
    "            subset = dataset[(dataset.modelname == modelname) & (dataset.exp_id == exp) & (dataset.sectors == sector)]\n",
    "        elif column is not None and condition is not None:\n",
    "            subset = dataset[(dataset.modelname == modelname) & (dataset.exp_id == exp) & (dataset.sectors == sector) & (dataset[column] == condition)]\n",
    "        else:\n",
    "            raise ValueError('Column and condition type must be the same (None & None, not None & not None).')\n",
    "        if not subset.empty:\n",
    "            scenarios.append([modelname, exp, sector])\n",
    "            all_trues.append(subset.true.to_numpy())\n",
    "            all_preds.append(subset.pred.to_numpy())\n",
    "            all_gp.append(subset.gp_pred.to_numpy())\n",
    "            all_std.append(subset.gp_pred.to_numpy())\n",
    "            \n",
    "            \n",
    "    return np.array(all_trues), np.array(all_preds), scenarios, np.array(all_gp), np.array(all_std)\n",
    "\n",
    "def get_uncertainty_bands(data: pd.DataFrame, confidence: str='95', quantiles: list[float]=[0.05, 0.95]):\n",
    "    \"\"\"Calculates uncertainty bands on the monte carlo dropout protocol. Includes traditional \n",
    "    confidence interval calculation as well as a quantile-based approach.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Dataframe or array of NXM, typically from ise.sectors.utils.data.group_by_run.\n",
    "        confidence (str, optional): Confidence level, must be in [95, 99]. Defaults to '95'.\n",
    "        quantiles (list[float], optional): Quantiles of uncertainty bands. Defaults to [0.05, 0.95].\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing [mean, sd, upper_ci, lower_ci, upper_q, lower_q], or the mean prediction, standard deviation, and the lower and upper confidence interval and quantile bands.\n",
    "    \"\"\"\n",
    "    z = {'95': 1.96, '99': 2.58}\n",
    "    data = np.array(data)\n",
    "    mean = data.mean(axis=0)\n",
    "    sd = np.sqrt(data.var(axis=0))\n",
    "    upper_ci = mean + (z[confidence] * (sd/np.sqrt(data.shape[0])))\n",
    "    lower_ci = mean - (z[confidence] * (sd/np.sqrt(data.shape[0])))\n",
    "    quantiles = np.quantile(data, quantiles, axis=0)\n",
    "    upper_q = quantiles[1,:]\n",
    "    lower_q = quantiles[0,:]\n",
    "    return mean, sd, upper_ci, lower_ci, upper_q, lower_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed04f2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trues, all_preds, scenarios, all_gp, all_std = group_by_run(dataset)\n",
    "\n",
    "mean_true, true_sd, true_upper_ci, true_lower_ci, true_upper_q, true_lower_q = get_uncertainty_bands(all_trues,)\n",
    "mean_pred, pred_sd, pred_upper_ci, pred_lower_ci, pred_upper_q, pred_lower_q = get_uncertainty_bands(all_preds,)\n",
    "mean_gp, gp_sd, gp_upper_ci, gp_lower_ci, gp_upper_q, gp_lower_q = get_uncertainty_bands(all_gp,)\n",
    "\n",
    "\n",
    "true_df = pd.DataFrame(all_trues).transpose()\n",
    "pred_df = pd.DataFrame(all_preds).transpose()\n",
    "gp_df = pd.DataFrame(all_gp).transpose()\n",
    "std_df = pd.DataFrame(all_std).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d24d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 3 year rolling average for GP\n",
    "rolled_gp = gp_df.rolling(3).mean()\n",
    "rolled_gp.loc[0, :] = gp_df.loc[0, :]\n",
    "rolled_gp.loc[1, :] = gp_df.loc[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74c414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# window = 3\n",
    "# rolled = np.array(pd.DataFrame(mean_gp).rolling(window).mean())\n",
    "# for i in range(window):\n",
    "#     rolled[i] = mean_gp[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3d964f",
   "metadata": {},
   "source": [
    "## Ensemble Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55c9d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "support = np.arange(2016, 2101)\n",
    "fig, axs = plt.subplots(1, 3, figsize=(21, 6), sharey=True, sharex=True)\n",
    "axs[0].plot(support, true_df, \n",
    "#             color='gray', \n",
    "            alpha=0.2)\n",
    "axs[0].plot(support, mean_true, \n",
    "            \"k-\", \n",
    "            linewidth=4, \n",
    "            label=\"Mean\")\n",
    "axs[1].plot(support, pred_df, \n",
    "#             color='gray', \n",
    "            alpha=0.2)\n",
    "axs[1].plot(support, mean_pred, \n",
    "            \"k-\", \n",
    "            linewidth=4, \n",
    "            label=\"Mean\")\n",
    "axs[2].plot(support, \n",
    "#             gp_df,\n",
    "            rolled_gp, \n",
    "#             color='gray', \n",
    "            alpha=0.2)\n",
    "axs[2].plot(support, mean_gp, \n",
    "            \"k-\", \n",
    "            linewidth=4, \n",
    "            label=\"Mean\")\n",
    "\n",
    "axs[1].plot(support, pred_upper_q, \"b--\", linewidth=3, label=\"5/95% Percentile (Predicted)\")\n",
    "axs[1].plot(support, pred_lower_q, \"b--\", linewidth=3)\n",
    "axs[0].plot(support, true_upper_q, \"b--\", linewidth=3, label=\"0.05/0.95 Quantile\")\n",
    "axs[0].plot(support, true_lower_q, \"b--\", linewidth=3)\n",
    "axs[2].plot(support, gp_upper_q, \"b--\", linewidth=3, label=\"0.05/0.95 Quantile\")\n",
    "axs[2].plot(support, gp_lower_q, \"b--\", linewidth=3)\n",
    "\n",
    "# axs[0].title.set_text(\"True\")\n",
    "axs[0].set_ylabel('Sea Level Contribution (mm SLE)', {'fontsize': 24})\n",
    "# axs[1].title.set_text(\"Predicted\")\n",
    "axs[0].text(2018, 21, 'A. Simulated (True) Ensemble', {'fontsize': 24, 'weight': 550})\n",
    "axs[1].text(2018, 21, 'B. NN Emulated (Predicted) Ensemble', {'fontsize': 24, 'weight': 550})\n",
    "axs[2].text(2018, 21, 'C. GP Emulated (Predicted) Ensemble', {'fontsize': 24, 'weight': 550})\n",
    "# axs[0].text(2090, 24, 'Comparison of Simulated (True) vs Emulated (Predicted) Ensemble')\n",
    "# axs[1].text(1997, 24, 'Comparison of Simulated (True) vs Emulated (Predicted) Ensemble')\n",
    "# plt.xlabel(\"Years since 2015\")\n",
    "# plt.suptitle(f\"Comparison of Simulated (True) vs Emulated (Predicted) Ensemble\")\n",
    "# plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "\n",
    "\n",
    "axs[0].yaxis.set_major_formatter(ScalarFormatter())\n",
    "axs[0].yaxis.set_minor_locator(AutoMinorLocator(4))\n",
    "axs[0].xaxis.set_minor_locator(AutoMinorLocator(4))\n",
    "axs[2].legend(frameon=False, loc='upper right',ncol=1,handlelength=4, fontsize=20)\n",
    "axs[0].set_xlim([2016, 2100])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "if save_figures:\n",
    "    plt.savefig('ensemble_with_gp.png', dpi=800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b82a6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "year2100 = pd.DataFrame(dict(true=true_df.iloc[84], nn=pred_df.iloc[84], gp=rolled_gp.iloc[84]))\n",
    "sns.boxplot(year2100)\n",
    "if save_figures:\n",
    "    plt.savefig('boxplots.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38463afa",
   "metadata": {},
   "source": [
    "## Mean and Distribution Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e347ba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_year = pd.DataFrame(dataset.groupby(by=['sectors', 'year']).mean())\n",
    "summed = sector_year.groupby(by='year').sum()\n",
    "\n",
    "rolled_gp = summed['gp_pred'].rolling(3).mean()\n",
    "rolled_gp[:3] = summed['gp_pred'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58c464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(17, 6.5), sharey=False, sharex=False)\n",
    "\n",
    "axs[0].plot(np.arange(2016, 2101), summed['true'], label='True', color=true_color, linewidth=3)\n",
    "axs[0].plot(np.arange(2016, 2101), summed['pred'], label='NN Predicted', color=nn_color, linewidth=2)\n",
    "# plt.plot(np.arange(2016, 2101), mean_gp, label='GP Predicted', color=gp_color)\n",
    "axs[0].plot(np.arange(2016, 2101), rolled_gp, label='GP Predicted', color=gp_color, linewidth=2)\n",
    "# axs[0].title('Ensemble Mean Sea Level Contribution over Time', {'fontsize': 'medium', 'weight': 650}, loc='right')\n",
    "axs[0].set_ylabel('Sea Level Contribution (mm SLE)', {'fontsize': 18})\n",
    "\n",
    "year = 2100\n",
    "true_dist, true_support = create_distribution(year=year, dataset=all_trues)\n",
    "pred_dist, pred_support = create_distribution(year=year, dataset=all_preds)\n",
    "gp_dist, gp_support = create_distribution(year=year, dataset=all_gp)\n",
    "\n",
    "axs[1].plot(true_support, true_dist, label=\"True\", color=true_color, linewidth=3)\n",
    "axs[1].plot(true_support, pred_dist, label=\"NN Predicted\", color=nn_color, linewidth=2)\n",
    "axs[1].plot(pred_support, gp_dist, label=\"GP Predicted\", color=gp_color, linewidth=2)\n",
    "# ax.title(\n",
    "#     f\"Distribution Comparison at year {year}, KL Divergence: {kl_divergence(pred_dist, true_dist):0.3f}\"\n",
    "# )\n",
    "\n",
    "axs[1].yaxis.set_major_formatter(ScalarFormatter())\n",
    "axs[1].yaxis.set_minor_locator(  AutoMinorLocator(2))\n",
    "axs[1].xaxis.set_minor_locator(  AutoMinorLocator(2))\n",
    "\n",
    "# ax.text(-31, .41, f\"NN KLD: {kl_divergence(pred_dist, true_dist):0.3f}\")\n",
    "# ax.text(-31, .41 - (.41-.38), f\"GP KLD: {kl_divergence(gp_dist, true_dist):0.3f}\")\n",
    "\n",
    "axs[0].text(2012.5, 14.65, 'A. Ensemble Mean Sea Level Contribution over Time', {'fontsize': 19, 'weight': 550})\n",
    "axs[1].text(-19.75, 0.2275, 'B. Comparison of Distributions at 2100', {'fontsize': 19, 'weight': 550})\n",
    "# axs[0].text(2016.5, 15.5, title1, {'fontsize': 18, 'weight': 550})\n",
    "\n",
    "axs[1].set_xlabel(\"Sea Level Contribution (mm SLE)\", {'fontsize': 18})\n",
    "axs[0].set_xlabel(\"Projection Year\", {'fontsize': 18})\n",
    "axs[1].set_ylabel(\"Density\", {'fontsize': 18})\n",
    "axs[1].legend(frameon=False, loc='upper left',ncol=1,handlelength=4)\n",
    "axs[0].legend(frameon=False, loc='upper left',ncol=1,handlelength=4)\n",
    "axs[1].set_xlim([-20, 16])\n",
    "\n",
    "if save_figures:\n",
    "    plt.savefig('mean-dist.png',)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f776f7a",
   "metadata": {},
   "source": [
    "## Error by Projection Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afde2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_year = dataset.groupby('year').mean()[['mse','gp_mse']]\n",
    "years = by_year.index\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "ax.plot(years, by_year['mse'], label=\"NN Predicted\", color=nn_color)\n",
    "ax.plot(years, by_year['gp_mse'], label=\"GP Predicted\", color=gp_color)\n",
    "\n",
    "ax.yaxis.set_major_formatter(ScalarFormatter())\n",
    "ax.yaxis.set_minor_locator(  AutoMinorLocator(2))\n",
    "ax.xaxis.set_minor_locator(  AutoMinorLocator(2))\n",
    "ax.text(2055, 11.18, f'Comparison of Emulator Error by Year', {'weight': 700})\n",
    "\n",
    "# ax.text(-19, .205, f\"NN KLD: {kl_divergence(pred_dist, true_dist):0.3f}\")\n",
    "# ax.text(-19, .19, f\"GP KLD: {kl_divergence(gp_dist, true_dist):0.3f}\")\n",
    "\n",
    "ax.set_xlabel(\"Projection Year\")\n",
    "ax.set_ylabel(\"Mean Squared Error (mm$^2$ SLE)\")\n",
    "ax.legend(frameon=False, loc='upper left',ncol=1,handlelength=4)\n",
    "# ax.set_xlim([-20, 20])\n",
    "\n",
    "if save_figures:\n",
    "    plt.savefig('error_by_year.png', dpi=800)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb6239e",
   "metadata": {},
   "source": [
    "## KL Divergence by Projection Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d976b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gp_metrics = []\n",
    "nn_metrics = []\n",
    "for year in range(2016, 2101):\n",
    "    true_dist, true_support = create_distribution(year=year, dataset=all_trues)\n",
    "    pred_dist, pred_support = create_distribution(year=year, dataset=all_preds)\n",
    "    gp_dist, gp_support = create_distribution(year=year, dataset=all_gp)\n",
    "    gp_metrics.append([kl_divergence(gp_dist, true_dist), js_divergence(gp_dist, true_dist)])\n",
    "    nn_metrics.append([kl_divergence(pred_dist, true_dist), js_divergence(pred_dist, true_dist)])\n",
    "\n",
    "gp_metrics = np.array(gp_metrics)\n",
    "nn_metrics = np.array(nn_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f729c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "ax.plot(np.arange(2016, 2101), gp_metrics[:, 0], label='GP', color=gp_color)\n",
    "ax.plot(np.arange(2016, 2101,), nn_metrics[:, 0], label='NN', color=nn_color)\n",
    "\n",
    "ax.yaxis.set_major_formatter(ScalarFormatter())\n",
    "ax.yaxis.set_minor_locator(  AutoMinorLocator(2))\n",
    "ax.xaxis.set_minor_locator(  AutoMinorLocator(2))\n",
    "ax.text(2034, 430, f'Kullback-Leibler divergence (KLD) by Projection Year', {'weight': 700})\n",
    "\n",
    "# ax.text(-31, .41, f\"NN KLD: {kl_divergence(pred_dist, true_dist):0.3f}\")\n",
    "# ax.text(-31, .41 - (.41-.38), f\"GP KLD: {kl_divergence(gp_dist, true_dist):0.3f}\")\n",
    "\n",
    "ax.set_ylabel(\"KL Divergence\")\n",
    "ax.legend(frameon=False, loc='upper right',ncol=1,handlelength=4)\n",
    "ax.set_xlim([2016, 2100])\n",
    "\n",
    "if save_figures:\n",
    "    plt.savefig('kld.png', dpi=800)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89327ce",
   "metadata": {},
   "source": [
    "## Results Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d06a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_time_sec = 708.282\n",
    "gp_time_sec = 7111.943440437317\n",
    "table = {\n",
    "    \"Neural Network\": [dataset.mse.mean(), dataset.mae.mean(), r2_score(dataset.true, dataset.pred), nn_time_sec/60, kl_divergence(pred_dist, true_dist), js_divergence(pred_dist, true_dist)],\n",
    "    \"Gaussian Process\": [dataset.gp_mse.mean(), dataset.gp_mae.mean(), r2_score(dataset.true, dataset.gp_pred), gp_time_sec/60, kl_divergence(gp_dist, true_dist), js_divergence(gp_dist, true_dist)],\n",
    "    \n",
    "}\n",
    "table = pd.DataFrame(table)\n",
    "table.index = ['Mean Squared Error', 'Mean Absolute Error', 'R^2', 'Time to train (min)', 'KL Divergence at 2100', 'JS Divergence at 2100']\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db66e7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binned_sle_table(\n",
    "    results_dataframe: pd.DataFrame,\n",
    "    bins,\n",
    "):\n",
    "    \"\"\"Creates table that analyzes loss functions over given ranges of SLE. Input is the results\n",
    "    dataframe from ise.sectors.utils.data.combine_testing_results. Note that bins can be an integer denoting\n",
    "    how many equal-width bins you want to cut the data into, or it can be a list of cutoffs. If the list does not\n",
    "    contain the mins and maxes of SLE in the dataset, it will be added automatically.\n",
    "    Args:\n",
    "        results_dataframe (pd.DataFrame): Testing results dataframe outputted from ise.sectors.utils.data.combine_testing_results\n",
    "        bins (list, optional): List of bin cutoffs or integer number of equal-width bins. Defaults to None.\n",
    "    Returns:\n",
    "        pd.DataFrame: Table of metrics per binned SLE.\n",
    "    \"\"\"\n",
    "    if not bins:\n",
    "        bins = 5\n",
    "\n",
    "    if not isinstance(bins, list) and not isinstance(bins, int):\n",
    "        raise AttributeError(\n",
    "            f\"bins type must be list[numeric] or int, received {type(bins)}\"\n",
    "        )\n",
    "\n",
    "    if isinstance(bins, list):\n",
    "        min_sle, max_sle = min(results_dataframe.true), max(results_dataframe.true)\n",
    "        if bins[0] != min_sle:\n",
    "            bins.insert(0, min_sle)\n",
    "        if bins[-1] != max_sle:\n",
    "            bins.append(max_sle)\n",
    "\n",
    "    results_dataframe[\"sle_bin\"], groups = pd.cut(\n",
    "        results_dataframe.true, bins, labels=None, retbins=True, include_lowest=True\n",
    "    )\n",
    "    mse_by_group = results_dataframe.groupby(\"sle_bin\").mean()[[\"mse\", \"mae\"]]\n",
    "    mse_by_group[\"Count\"] = results_dataframe.groupby(\"sle_bin\").count()[\"true\"]\n",
    "    mse_by_group[\"Prop\"] = (mse_by_group[\"Count\"] / len(results_dataframe)) * 100\n",
    "    mse_by_group[\"Prop\"] = round(mse_by_group[\"Prop\"], 4).astype(str) + \"%\"\n",
    "    mse_by_group.index = [\n",
    "        f\"Between {val:0.2f} and {groups[i+1]:0.2f} mm SLE\"\n",
    "        for i, val in enumerate(groups[:-1])\n",
    "    ]\n",
    "    mse_by_group.columns = [\n",
    "        \"Mean Squared Error\",\n",
    "        \"Mean Absolute Error\",\n",
    "        \"Count in Test Dataset\",\n",
    "        \"Proportion in Test Dataset\",\n",
    "    ]\n",
    "\n",
    "    return pd.DataFrame(mse_by_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caae614",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_sle_table(dataset, bins=[-15, -8, 0, 8, 15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2bbe61",
   "metadata": {},
   "source": [
    "## Example Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3d39c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6), sharey=True, sharex=True)\n",
    "\n",
    "chunk = dataset[(dataset.modelname == \"IMAU_IMAUICE1\") & (dataset.exp_id == 'exp09') & (dataset.sectors == 0.2352941176470588)]\n",
    "chunkb = dataset[(dataset.modelname == \"IMAU_IMAUICE1\") & (dataset.exp_id == 'exp12') & (dataset.sectors == 0.7058823529411764)]\n",
    "# chunk = dataset[(dataset.modelname == \"PIK_PISM1\") & (dataset.exp_id == 'exp04') & (round(dataset.sectors, 8) == 0.17647059)]\n",
    "# chunkb = dataset[(dataset.modelname == \"PIK_PISM1\") & (dataset.exp_id == 'exp01') & (dataset.sectors == 0.7058823529411764)]\n",
    "\n",
    "\n",
    "\n",
    "axs[1].plot(chunk.year, chunk.true, label='True', color=true_color, linewidth=4)\n",
    "axs[1].plot(chunk.year, chunk.pred, label='NN Predicted', color=nn_color, linewidth=3)\n",
    "axs[1].plot(chunk.year, chunk.gp_pred, label='GP Predicted', color=gp_color, linewidth=3)\n",
    "axs[1].fill_between(\n",
    "    chunk.year,\n",
    "    chunk.gp_pred - 1.96 * chunk.gp_std,\n",
    "    chunk.gp_pred + 1.96 * chunk.gp_std,\n",
    "    alpha=0.2,\n",
    "    color=gp_color,\n",
    ")\n",
    "axs[1].fill_between(\n",
    "    chunk.year,\n",
    "    chunk.lower_bound,\n",
    "    chunk.upper_bound,\n",
    "    alpha=0.2,\n",
    "    color=nn_color,\n",
    ")\n",
    "\n",
    "axs[0].plot(chunkb.year, chunkb.true, label='True', color=true_color, linewidth=4)\n",
    "axs[0].plot(chunkb.year, chunkb.pred, label='NN Predicted', color=nn_color, linewidth=3)\n",
    "axs[0].plot(chunkb.year, chunkb.gp_pred, label='GP Predicted', color=gp_color, linewidth=3)\n",
    "axs[0].fill_between(\n",
    "    chunkb.year,\n",
    "    chunkb.gp_pred - 1.96 * chunkb.gp_std,\n",
    "    chunkb.gp_pred + 1.96 * chunkb.gp_std,\n",
    "    alpha=0.2,\n",
    "    color=gp_color,\n",
    ")\n",
    "axs[0].fill_between(\n",
    "    chunkb.year,\n",
    "    chunkb.lower_bound,\n",
    "    chunkb.upper_bound,\n",
    "    alpha=0.2,\n",
    "    color=nn_color,\n",
    ")\n",
    "title = chunk.iloc[0]\n",
    "title1 = f\"A. {title.modelname}, {title.exp_id}, sector 5\"\n",
    "title0 = f\"B. {title.modelname}, exp12, sector 13\"\n",
    "\n",
    "axs[0].yaxis.set_major_formatter(ScalarFormatter())\n",
    "axs[0].yaxis.set_minor_locator(  AutoMinorLocator(2))\n",
    "axs[0].xaxis.set_minor_locator(  AutoMinorLocator(2))\n",
    "axs[1].text(2016.5, 16, title0, {'fontsize': 18, 'weight': 550})\n",
    "axs[0].text(2016.5, 16, title1, {'fontsize': 18, 'weight': 550})\n",
    "first_metrics = f\"\"\"    MSE:\n",
    "NN: {chunk.mse.mean():0.3f}\n",
    "GP: {chunk.gp_mse.mean():0.3f}\"\"\"\n",
    "second_metrics = f\"\"\"    MSE:\n",
    "NN: {chunkb.mse.mean():0.3f}\n",
    "GP: {chunkb.gp_mse.mean():0.3f}\"\"\"\n",
    "axs[0].text(2019, -5, first_metrics, {'fontsize': 18, 'weight': 200})\n",
    "axs[1].text(2019, -5, second_metrics, {'fontsize': 18, 'weight': 200})\n",
    "\n",
    "\n",
    "axs[0].set_ylabel(\"Sea Level Contribution (mm SLE)\")\n",
    "# ax.set_ylabel(\"Density\")\n",
    "axs[0].set_xlim([2016, 2100])\n",
    "axs[0].legend(frameon=False, loc='upper left',ncol=1,handlelength=4)\n",
    "if save_figures:\n",
    "    plt.savefig('example_proj_0.4.svg')\n",
    "plt.show()\n",
    "\n",
    "# IMAU_IMAUICE1, exp09, 0.2352941176470588\n",
    "# IMAU_IMAUICE1, exp12, 0.7058823529411764    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fecf294",
   "metadata": {},
   "source": [
    "## Calibrated Uncertainty Interval Widths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f35603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibration(scale, calibration, pred, sd, true,):\n",
    "    scaled_upper = pred + scale*sd\n",
    "    scaled_lower = pred - scale*sd\n",
    "    in_bounds = (true >= scaled_lower) & (true <= scaled_upper)\n",
    "    return np.mean(in_bounds) - calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d128fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['in_bounds'] = (dataset.true >= dataset.lower_bound) & (dataset.true <= dataset.upper_bound)\n",
    "\n",
    "from scipy.optimize import fsolve\n",
    "certainty = 0.95\n",
    "nn_scale = fsolve(calibration, x0=6, args=(certainty, dataset.pred, dataset.sd, dataset.true))\n",
    "gp_scale = fsolve(calibration, x0=2, args=(certainty, dataset.gp_pred, dataset.gp_std, dataset.true))\n",
    "print(f\"\"\"NN Scale: {nn_scale}\n",
    "GP Scale: {gp_scale}\"\"\")\n",
    "\n",
    "scale = nn_scale[0]\n",
    "dataset['scaled_upper'] = dataset.pred + scale*dataset.sd\n",
    "dataset['scaled_lower'] = dataset.pred - scale*dataset.sd\n",
    "dataset['in_scaled_bounds'] = (dataset.true >= dataset.scaled_lower) & (dataset.true <= dataset.scaled_upper)\n",
    "print(f\"NN Calibration: {np.mean(dataset.in_scaled_bounds)}\")\n",
    "\n",
    "scale = gp_scale[0]\n",
    "dataset['gp_upper'] = dataset.gp_pred + scale * dataset.gp_std\n",
    "dataset['gp_lower'] = dataset.gp_pred - scale * dataset.gp_std\n",
    "dataset['gp_in_bounds'] = (dataset.true >= dataset.gp_lower) & (dataset.true <= dataset.gp_upper)\n",
    "print(f\"GP Calibration: {np.mean(dataset.gp_in_bounds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2941ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2100\n",
    "nn, gp = [], []\n",
    "support = np.arange(0.5, 1, 0.01)\n",
    "support = [.5, .55, .6, .65, .7, .75, .8, .85, .9, .95,]\n",
    "for i in support:\n",
    "    nn_scale = fsolve(calibration, x0=6, args=(i, dataset.pred, dataset.sd, dataset.true))\n",
    "    gp_scale = fsolve(calibration, x0=2, args=(i, dataset.gp_pred, dataset.gp_std, dataset.true))\n",
    "    \n",
    "    scale = nn_scale[0]\n",
    "    dataset['scaled_upper'] = dataset.pred + scale*dataset.sd\n",
    "    dataset['scaled_lower'] = dataset.pred - scale*dataset.sd\n",
    "    dataset['in_scaled_bounds'] = (dataset.true >= dataset.scaled_lower) & (dataset.true <= dataset.scaled_upper)\n",
    "\n",
    "    scale = gp_scale[0]\n",
    "    dataset['gp_upper'] = dataset.gp_pred + scale * dataset.gp_std\n",
    "    dataset['gp_lower'] = dataset.gp_pred - scale * dataset.gp_std\n",
    "    dataset['gp_in_bounds'] = (dataset.true >= dataset.gp_lower) & (dataset.true <= dataset.gp_upper)\n",
    "    \n",
    "    data = dataset[dataset.year == year].copy()\n",
    "    bound_width = data.scaled_upper - data.scaled_lower\n",
    "    gp_bound_width = data.gp_upper - data.gp_lower\n",
    "    nn.append(bound_width.mean())\n",
    "    gp.append(gp_bound_width.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c533ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "ax.plot(support, nn, label='NN', color=nn_color, marker='o')\n",
    "ax.plot(support, gp, label='GP', color=gp_color, marker='o')\n",
    "\n",
    "ax.yaxis.set_major_formatter(ScalarFormatter())\n",
    "ax.yaxis.set_minor_locator(  AutoMinorLocator(2))\n",
    "ax.xaxis.set_minor_locator(  AutoMinorLocator(6))\n",
    "ax.text(0.570, 14.55, \"Calibrated Uncertainty Width vs Confidence Level\", {'fontsize': 18, 'weight': 550})\n",
    "\n",
    "\n",
    "ax.set_ylabel(\"Emulator Uncertainty Interval Width (mm SLE)\")\n",
    "ax.set_xlabel(\"Calibrated Confidence Level\")\n",
    "ax.legend(frameon=False, loc='upper left',ncol=1,handlelength=4)\n",
    "if save_figures:\n",
    "    plt.savefig('uncertainty_width.png', dpi=800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34acffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bound_width = dataset.scaled_upper - dataset.scaled_lower\n",
    "gp_bound_width = dataset.gp_upper - dataset.gp_lower\n",
    "print(f\"Average NN Bound Width: {bound_width.mean()}\")\n",
    "print(f\"Average GP Bound Width: {gp_bound_width.mean()}\")\n",
    "\n",
    "year = 2100\n",
    "data = dataset[dataset.year == year].copy()\n",
    "bound_width = data.scaled_upper - data.scaled_lower\n",
    "gp_bound_width = data.gp_upper - data.gp_lower\n",
    "print(f\"Average NN Bound Width at {year}: {bound_width.mean()}\")\n",
    "print(f\"Average GP Bound Width at {year}: {gp_bound_width.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3b4e1b",
   "metadata": {},
   "source": [
    "## Mean Emulated vs Simulated SLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9759951",
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = plt.cm.nipy_spectral\n",
    "colors = colormap(np.linspace(0, 1, 18))\n",
    "ax.set_prop_cycle('color', colors)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(17, 6.5), sharey=True, sharex=False)\n",
    "\n",
    "true = pd.DataFrame(dataset.groupby(by=['year', 'sectors']).mean()['true']).reset_index()\n",
    "\n",
    "number_of_plots=18\n",
    "colormap = plt.cm.tab20b #I suggest to use nipy_spectral, Set1,Paired\n",
    "axs[0].set_prop_cycle('color', [colormap(i) for i in np.linspace(0, 1,number_of_plots)])\n",
    "axs[1].set_prop_cycle('color', [colormap(i) for i in np.linspace(0, 1,number_of_plots)])\n",
    "\n",
    "\n",
    "for i in set(true.sectors):\n",
    "    p = true[true.sectors == i]\n",
    "    axs[0].plot(np.arange(2016, 2101,), p.true, label=str(i))\n",
    "\n",
    "pred = pd.DataFrame(dataset.groupby(by=['year', 'sectors']).mean()['pred']).reset_index()\n",
    "\n",
    "for i in set(pred.sectors):\n",
    "    p = pred[pred.sectors == i]\n",
    "    axs[1].plot(np.arange(2016, 2101,), p.pred)\n",
    "\n",
    "axs[0].set_ylabel('Sea Level Contribution (mm SLE)', {'fontsize': 18})\n",
    "axs[1].yaxis.set_major_formatter(ScalarFormatter())\n",
    "axs[1].yaxis.set_minor_locator(  AutoMinorLocator(2))\n",
    "axs[1].xaxis.set_minor_locator(  AutoMinorLocator(2))\n",
    "\n",
    "axs[0].text(2012.5, 6.5, 'A. True Mean SLE by Sector', {'fontsize': 19, 'weight': 550})\n",
    "axs[1].text(2012.5, 6.5, 'B. Emulated (NN) Mean SLE by Sector', {'fontsize': 19, 'weight': 550})\n",
    "\n",
    "# axs[1].set_xlabel(\"Sea Level Contribution (mm SLE)\", {'fontsize': 18})\n",
    "axs[0].set_xlabel(\"Projection Year\", {'fontsize': 18})\n",
    "axs[1].set_xlabel(\"Projection Year\", {'fontsize': 18})\n",
    "# axs[1].set_ylabel(\"Density\", {'fontsize': 18})\n",
    "# axs[1].set_ylabel(\"Density\", {'fontsize': 18})\n",
    "# axs[1].legend(frameon=False, loc='upper left',ncol=1,handlelength=4)\n",
    "axs[0].legend(frameon=False, loc='upper left',ncol=1,handlelength=4)\n",
    "# axs[1].set_xlim([-20, 16])\n",
    "\n",
    "if save_figures:\n",
    "    plt.savefig('mean_by_sector.svg',)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5098f024",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

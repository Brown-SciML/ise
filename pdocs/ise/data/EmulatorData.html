<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>ise.data.EmulatorData API documentation</title>
<meta name="description" content="Module containing EmulatorData class with all associated methods and attributes. Primarily carries out data loading, feature engineering &amp; processing …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ise.data.EmulatorData</code></h1>
</header>
<section id="section-intro">
<p>Module containing EmulatorData class with all associated methods and attributes. Primarily carries out data loading, feature engineering &amp; processing of formatted data.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Module containing EmulatorData class with all associated methods and attributes. Primarily carries out data loading, feature engineering &amp; processing of formatted data.&#34;&#34;&#34;
import random
import pandas as pd
import numpy as np
from sklearn import preprocessing as sp
np.random.seed(10)


class EmulatorData:
    &#34;&#34;&#34;Class containing attributes and methods for storing and handling ISMIP6 ice sheet data.&#34;&#34;&#34;
    def __init__(self, directory: str):
        &#34;&#34;&#34;Initializes class and opens/stores data. Includes loading processed files from 
        ise.data.processors functions, converting IVAF to SLE, and saving initial condition data
        for later use.

        Args:
            directory (str): Directory containing processed files from ise.data.processors functions. Should contain master.csv
        &#34;&#34;&#34;

        self.directory = directory

        try:
            self.data = pd.read_csv(f&#34;{self.directory}/master.csv&#34;, low_memory=False)
        except FileNotFoundError:
            try:
                self.inputs = pd.read_csv(f&#34;{self.directory}/inputs.csv&#34;)
                self.outputs = pd.read_csv(f&#34;{self.directory}/outputs.csv&#34;)
            except FileNotFoundError:
                raise FileNotFoundError(&#39;Files not found, make sure to run all processing functions.&#39;)

        # convert to SLE
        self.data[&#39;sle&#39;] = self.data.ivaf / 1e9 / 362.5  # m^3 / 1e9 / 362.5 = Gt / 1 mm --&gt; mm SLE
        self.data[&#39;modelname&#39;] = self.data.groupname + &#39;_&#39; + self.data.modelname


        # Save data on initial conditions for use in data splitting
        unique_batches = self.data.groupby([&#39;modelname&#39;, &#39;sectors&#39;, &#39;exp_id&#39;]).size().reset_index().rename(
            columns={0: &#39;count&#39;}).drop(columns=&#39;count&#39;)
        self.batches = unique_batches.values.tolist()
        self.output_columns = [&#39;icearea&#39;, &#39;iareafl&#39;, &#39;iareagr&#39;, &#39;ivol&#39;, &#39;ivaf&#39;, &#39;smb&#39;, &#39;smbgr&#39;, &#39;bmbfl&#39;, &#39;sle&#39;]

        for col in [&#39;icearea&#39;, &#39;iareafl&#39;, &#39;iareagr&#39;, &#39;ivol&#39;, &#39;smb&#39;, &#39;smbgr&#39;, &#39;bmbfl&#39;, &#39;sle&#39;]:
            self.data[col] = self.data[col].fillna(self.data[col].mean())

        self.X = None
        self.y = None
        self.scaler_X = None
        self.scaler_y = None

    def process(self, target_column: str=&#39;sle&#39;, drop_missing: bool=True, 
                drop_columns: list[str]=True, boolean_indices: bool=True, scale: bool=True,
                split_type: str=&#39;batch&#39;, drop_outliers: str=False, drop_expression: list[tuple]=None,
                time_series: bool=False, lag: int=None):
        &#34;&#34;&#34;Carries out feature engineering &amp; processing of formatted data. Includes dropping missing
        values, eliminating columns, creating boolean indices of categorical variables, scaling, 
        dataset splitting, and more. Refer to the individual functions contained in the source
        code for more information on each process.

        Args:
            target_column (str, optional): Column to be predicted. Defaults to &#39;sle&#39;.
            drop_missing (bool, optional): Flag denoting whether to drop missing values. Defaults to True.
            drop_columns (list[str], optional): List containing which columns (variables) to be dropped. Should be List[str] or boolean. If True is chosen, columns are dropped that will result in optimal performance. Defaults to True.
            boolean_indices (bool, optional): Flag denoting whether to create boolean indices for all categorical variables left after dropping columns. Defaults to True.
            scale (bool, optional): Flag denoting whether to scale data between zero and 1.  Sklearn&#39;s MinMaxScaler is used. Defaults to True.
            split_type (str, optional): Method to split data into training and testing set, must be in [random, batch]. Random is not recommended but is included for completeness. Defaults to &#39;batch&#39;.
            drop_outliers (str, optional): Method by which outliers will be dropped, must be in [quantile, explicit]. Defaults to False.
            drop_expression (list[tuple], optional): Expressions by which to drop outliers, see EmulatorData.drop_outliers. If drop_outliers==quantile, drop_expression must be a list[float] containing quantile bounds. Defaults to None. 
            time_series (bool, optional): Flag denoting whether to process the data as a time-series dataset or traditional non-time dataset. Defaults to False.
            lag (int, optional): Lag variable for time-series processing. Defaults to None.
            
        Returns:
            tuple: Multi-output returning [EmulatorData, train_features, test_features, train_labels, test_labels]
        &#34;&#34;&#34;


        if time_series:
            if lag is None:
                raise ValueError(&#39;If time_series == True, lag cannot be None&#39;)
            time_dependent_columns = [&#39;salinity&#39;, &#39;temperature&#39;, &#39;thermal_forcing&#39;,
                                      &#39;pr_anomaly&#39;, &#39;evspsbl_anomaly&#39;, &#39;mrro_anomaly&#39;, &#39;smb_anomaly&#39;,
                                      &#39;ts_anomaly&#39;,]
            separated_dfs = [y for x, y in self.data.groupby([&#39;sectors&#39;, &#39;exp_id&#39;, &#39;modelname&#39;])]
            for df in separated_dfs:
                for shift in range(1, lag + 1):
                    for column in time_dependent_columns:
                        df[f&#34;{column}.lag{shift}&#34;] = df[column].shift(shift, fill_value=0)
            self.data = pd.concat(separated_dfs)


        if drop_columns:
            if drop_columns is True:
                self.drop_columns(columns=[&#39;experiment&#39;, &#39;exp_id&#39;, &#39;groupname&#39;, &#39;regions&#39;])
            elif isinstance(drop_columns, list):
                self.drop_columns(columns=drop_columns)
            else:
                raise ValueError(f&#39;drop_columns argument must be of type boolean|list, received {type(drop_columns)}&#39;)

        if drop_missing:
            self = self.drop_missing()

        if boolean_indices:
            self = self.create_boolean_indices()

        if drop_outliers:
            if drop_outliers.lower() == &#39;quantile&#39;:
                self = self.drop_outliers(method=drop_outliers, quantiles=drop_expression)
            elif drop_outliers.lower() == &#39;explicit&#39;:
                self = self.drop_outliers(method=drop_outliers, expression=drop_expression)
            else:
                raise ValueError(&#39;drop_outliers argument must be in [quantile, explicit]&#39;)
            
            

        self = self.split_data(target_column=target_column)

        if scale:
            self.X = self.scale(self.X, &#39;inputs&#39;, scaler=&#39;MinMaxScaler&#39;)
            if target_column == &#39;sle&#39;:
                self.y = np.array(self.y)
            else:
                self.y = self.scale(self.y, &#39;outputs&#39;, scaler=&#39;MinMaxScaler&#39;)

        self = self.train_test_split(split_type=split_type)

        return self, self.train_features, self.test_features, self.train_labels, self.test_labels

    def drop_outliers(self, method: str, expression: list[tuple]=None, quantiles: list[float]=[0.01, 0.99]):
        &#34;&#34;&#34;Drops simulations that are outliers based on the provided method and expression. 
        Extra complexity is handled due to the necessity of removing the entire 85 row series from 
        the dataset rather than simply removing the rows with given conditions. Note that the 
        condition indicates rows to be DROPPED, not kept (e.g. &#39;sle&#39;, &#39;&gt;&#39;, &#39;20&#39; would drop all
        simulations containing sle values over 20). If quantile method is used, outliers are dropped
        from the SLE column based on the provided quantile in the quantiles argument. If explicit is
        chosen, expression must contain a list of tuples such that the tuple contains 
        [(column, operator, expression)] of the subset, e.g. [(&#34;sle&#34;, &#34;&gt;&#34;, 20), (&#34;sle&#34;, &#34;&lt;&#34;, -20)].

        Args:
            method (str): Method of outlier deletion, must be in [quantile, explicit]
            expression (list[tuple]): List of subset expressions in the form [column, operator, value], defaults to None.
            quantiles (list[float]): , defaults to [0.01, 0.99].

        Returns:
            EmulatorData: self, with self.data having outliers dropped.
        &#34;&#34;&#34;
        
        if method.lower() == &#39;quantile&#39;:
            if quantiles is None:
                raise AttributeError(&#39;If method == quantile, quantiles argument cannot be None&#39;)
            lower_sle, upper_sle = np.quantile(np.array(self.data.sle), quantiles)
            outlier_data = self.data[(self.data[&#39;sle&#39;] &lt;= lower_sle) | (self.data[&#39;sle&#39;] &gt;= upper_sle)]
        elif method.lower() == &#39;explicit&#39;:
            
            if expression is None:
                raise AttributeError(&#39;If method == explicit, expression argument cannot be None&#39;)
            elif not isinstance(expression, list) or not isinstance(expression[0], tuple):
                raise AttributeError(&#39;Expression argument must be a list of tuples, e.g. [(&#34;sle&#34;, &#34;&gt;&#34;, 20), (&#34;sle&#34;, &#34;&lt;&#34;, -20)]&#39;)
            
            outlier_data = self.data
            for subset_expression in expression:
                column, operator, value = subset_expression            

                if operator.lower() in (&#39;equal&#39;, &#39;equals&#39;, &#39;=&#39;, &#39;==&#39;):
                    outlier_data = outlier_data[outlier_data[column] == value]
                elif operator.lower() in (&#39;not equal&#39;, &#39;not equals&#39;, &#39;!=&#39;, &#39;~=&#39;):
                    outlier_data = outlier_data[outlier_data[column] != value]
                elif operator.lower() in (&#39;greather than&#39;, &#39;greater&#39;, &#39;&gt;=&#39;, &#39;&gt;&#39;):
                    outlier_data = outlier_data[outlier_data[column] &gt; value]
                elif operator.lower() in (&#39;less than&#39;, &#39;less&#39;, &#39;&lt;=&#39;, &#39;&lt;&#39;):
                    outlier_data = outlier_data[outlier_data[column] &lt; value]
                else:
                    raise ValueError(f&#39;Operator must be in [\&#34;==\&#34;, \&#34;!=\&#34;, \&#34;&gt;\&#34;, \&#34;&lt;\&#34;], received {operator}&#39;)

        cols = outlier_data.columns
        nonzero_columns = outlier_data.apply(lambda x: x &gt; 0).apply(lambda x: list(cols[x.values]), axis=1)

        # Create dataframe of experiments with outliers (want to delete the entire 85 rows)
        outlier_runs = pd.DataFrame()
        outlier_runs[&#39;modelname&#39;] = nonzero_columns.apply(lambda x: x[-6])
        outlier_runs[&#39;exp_id&#39;] = nonzero_columns.apply(lambda x: x[-5])
        outlier_runs[&#39;sectors&#39;] = outlier_data.sectors
        outlier_runs_list = outlier_runs.values.tolist()
        unique_outliers = [list(x) for x in set(tuple(x) for x in outlier_runs_list)]

        # Drop those runs
        for i in unique_outliers:
            modelname = i[0]
            exp_id = i[1]
            sector = i[2]
            self.data = self.data.drop(
                self.data[(self.data[modelname] == 1) &amp; (self.data[exp_id] == 1) &amp; (self.data.sectors == sector)].index
            )

        return self

    def split_data(self, target_column: str, ):
        &#34;&#34;&#34;Splits data into features and labels based on target column.

        Args:
            target_column (str): Output column to be predicted.

        Returns:
            EmulatorData: self, with self.X and self.y as attributes.
        &#34;&#34;&#34;
        self.target_column = target_column
        self.X = self.data.drop(columns=self.output_columns)
        self.y = self.data[target_column]
        self.input_columns = self.X.columns
        return self

    def train_test_split(self, train_size: float=0.7, split_type: str=&#39;batch&#39;):
        &#34;&#34;&#34;Splits dataset into training set and testing set. Can be split using two different
        methods: random and batch. The random method splits by randomly sampling rows, whereas 
        batch method randomly samples entire simulation series (85 rows) in order to keep simulations
        together during testing. Random method is included for completeness but is not recommended
        for use in emulator creation.

        Args:
            train_size (float, optional): Proportion of data in training set, between 0 and 1. Defaults to 0.7.
            split_type (str, optional): Splitting method, must be in [random, batch]. Defaults to &#39;batch&#39;.

        Returns:
            EmulatorData: self, with self.train_features, self.test_features, self.train_labels, self.test_labels as attributes.
        &#34;&#34;&#34;

        if not isinstance(self.X, pd.DataFrame):
            self.X = pd.DataFrame(self.X, columns=self.input_columns)

        if &#39;random&#39; in split_type.lower():
            self.train_features = self.X.sample(frac=train_size, random_state=0)
            training_indices = self.train_features.index
            self.train_labels = self.y[training_indices].squeeze()

            self.test_features = self.X.drop(training_indices)
            self.test_labels = pd.Series(self.y.squeeze()).drop(training_indices)


        elif split_type.lower() == &#34;batch&#34;:
            # batch -- grouping of 85 years of a particular model, experiment, and sector
            # Calculate how many batches you&#39;ll need (roughly) for train/test proportion
            test_num_rows = len(self.X) * (1 - train_size)
            num_years = len(set(self.data.year))
            num_test_batches = test_num_rows // num_years

            # Get all possible values for sector, experiment, and model
            all_sectors = list(set(self.X.sectors))
            all_experiments = [col for col in self.X.columns if &#34;exp_id&#34; in col]
            all_modelnames = [col for col in self.X.columns if &#34;modelname&#34; in col]

            # Set up concatenation of test data scenarios...
            test_scenarios = []
            test_dataset = pd.DataFrame()

            # Keep this running until you have enough samples
            np.random.seed(10)
            while len(test_scenarios) &lt; num_test_batches:
                # Get a random
                random_model = np.random.choice(all_modelnames)
                random_sector = np.random.choice(all_sectors)
                random_experiment = np.random.choice(all_experiments)
                test_scenario = [random_model, random_sector, random_experiment]
                if test_scenario not in test_scenarios:
                    scenario_df = self.X[(self.X[random_model] == 1) &amp; (self.X[&#39;sectors&#39;] == random_sector) &amp; (
                            self.X[random_experiment] == 1)]
                    if not scenario_df.empty:
                        test_scenarios.append(test_scenario)
                        test_dataset = pd.concat([test_dataset, scenario_df])
            self.test_features = test_dataset
            testing_indices = self.test_features.index
            self.test_labels = self.y[testing_indices].squeeze()

            self.train_features = self.X.drop(testing_indices)
            self.train_labels = pd.Series(self.y.squeeze()).drop(testing_indices)

            self.test_scenarios = test_scenarios
        
        else:
            raise(f&#39;split_type must be in [random, batch], received {split_type}&#39;)

        return self

    def drop_missing(self):
        &#34;&#34;&#34;Drops rows with missing values (wrapper for pandas.DataFrame.dropna()).

        Returns:
            EmulatorData: self, with NA values dropped from self.data.
        &#34;&#34;&#34;
        self.data = self.data.dropna()
        return self

    def create_boolean_indices(self, columns: str=&#39;all&#39;):
        &#34;&#34;&#34;Creates boolean indices (one hot encoding) for categoritcal variables in columns 
        argument. Wrapper for pandas.get_dummies() with added functionality for prefix separation.

        Args:
            columns (str, optional): Categorical variables to be encoded. Defaults to &#39;all&#39;.

        Returns:
            EmulatorData: self, with boolean indices in self.data.
        &#34;&#34;&#34;
        if columns == &#39;all&#39;:
            self.data = pd.get_dummies(self.data, prefix_sep=&#34;-&#34;)
        else:
            if not isinstance(columns, list):
                raise ValueError(f&#39;Columns argument must be of type: list, received {type(columns)}.&#39;)

            self.data = pd.get_dummies(self.data, columns=columns, prefix_sep=&#34;-&#34;)

            for col in self.data.columns:
                self.data[col] = self.data[col].astype(float)
        return self

    def drop_columns(self, columns: list[str]):
        &#34;&#34;&#34;Drops columns in columns argument from the dataset. Wrapper for pandas.DataFrame.drop()
        with error checking.

        Args:
            columns (list[str]): List of columns (or singular string column) to be dropped from the dataset.

        Returns:
            EmulatorData: self, with desired columns dropped from self.data.
        &#34;&#34;&#34;
        if not isinstance(columns, list) and not isinstance(columns, str):
            raise ValueError(f&#39;Columns argument must be of type: str|list, received {type(columns)}.&#39;)
        columns = list(columns)

        self.data = self.data.drop(columns=columns)

        return self

    def scale(self, values: pd.DataFrame, values_type: str, scaler: str=&#34;MinMaxScaler&#34;):
        &#34;&#34;&#34;Scales dataframe and saves scaler for future use in unscaling. Sklearn&#39;s scaling API is
        used. MinMaxScaler is recommended but StandardScaler is also supported.

        Args:
            values (pd.DataFrame): Dataframe to be scaled.
            values_type (str): Whether the dataframe to be scaled is a feature or labels dataframe, must be in [inputs, outputs]
            scaler (str, optional): Type of scaler to be used, must be in [MinMaxScaler, StandardScaler]. Defaults to &#34;MinMaxScaler&#34;.

        Returns:
            pd.DataFrame: scaled dataset with self.scaler_X and self.scaler_y as attributes in the EmulatorData class.
        &#34;&#34;&#34;
        if self.X is None and self.y is None:
            raise AttributeError(&#39;Data must be split before scaling using model.split_data method.&#39;)

        if &#34;minmax&#34; in scaler.lower():
            if &#39;input&#39; in values_type.lower():
                self.scaler_X = sp.MinMaxScaler()
            else:
                self.scaler_y = sp.MinMaxScaler()
        elif &#34;standard&#34; in scaler.lower():
            if &#39;input&#39; in values_type.lower():
                self.scaler_X = sp.StandardScaler()
            else:
                self.scaler_y = sp.StandardScaler()
        else:
            raise ValueError(f&#39;scaler argument must be in [\&#39;MinMaxScaler\&#39;, \&#39;StandardScaler\&#39;], received {scaler}&#39;)

        if &#39;input&#39; in values_type.lower():
            self.input_columns = self.X.columns
            self.scaler_X.fit(self.X)
            return pd.DataFrame(self.scaler_X.transform(values), columns=self.X.columns)

        # TODO: Don&#39;t need this anymore with SLE as the prediction
        elif &#39;output&#39; in values_type.lower():
            self.scaler_y.fit(np.array(self.y).reshape(-1, 1))
            return self.scaler_y.transform(np.array(values).reshape(-1, 1))

        else:
            raise ValueError(f&#34;values_type must be in [&#39;inputs&#39;, &#39;outputs&#39;], received {values_type}&#34;)

    def unscale(self, values: pd.DataFrame, values_type: str):
        &#34;&#34;&#34;Unscales data based on scalers trained in EmulatorData.scale().

        Args:
            values (pd.DataFrame): Dataframe to be unscaled.
            values_type (str): Whether the dataframe to be unscaled is a feature or labels dataframe, must be in [inputs, outputs]

        Returns:
           pd.DataFrame: unscaled dataset.
        &#34;&#34;&#34;

        if &#39;input&#39; in values_type.lower():
            return pd.DataFrame(self.scaler_X.inverse_transform(values), columns=self.input_columns)

        elif &#39;output&#39; in values_type.lower():
            return self.scaler_y.inverse_transform(values.reshape(-1, 1))

        else:
            raise ValueError(f&#34;values_type must be in [&#39;inputs&#39;, &#39;outputs&#39;], received {values_type}&#34;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ise.data.EmulatorData.EmulatorData"><code class="flex name class">
<span>class <span class="ident">EmulatorData</span></span>
<span>(</span><span>directory: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Class containing attributes and methods for storing and handling ISMIP6 ice sheet data.</p>
<p>Initializes class and opens/stores data. Includes loading processed files from
ise.data.processors functions, converting IVAF to SLE, and saving initial condition data
for later use.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>directory</code></strong> :&ensp;<code>str</code></dt>
<dd>Directory containing processed files from ise.data.processors functions. Should contain master.csv</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EmulatorData:
    &#34;&#34;&#34;Class containing attributes and methods for storing and handling ISMIP6 ice sheet data.&#34;&#34;&#34;
    def __init__(self, directory: str):
        &#34;&#34;&#34;Initializes class and opens/stores data. Includes loading processed files from 
        ise.data.processors functions, converting IVAF to SLE, and saving initial condition data
        for later use.

        Args:
            directory (str): Directory containing processed files from ise.data.processors functions. Should contain master.csv
        &#34;&#34;&#34;

        self.directory = directory

        try:
            self.data = pd.read_csv(f&#34;{self.directory}/master.csv&#34;, low_memory=False)
        except FileNotFoundError:
            try:
                self.inputs = pd.read_csv(f&#34;{self.directory}/inputs.csv&#34;)
                self.outputs = pd.read_csv(f&#34;{self.directory}/outputs.csv&#34;)
            except FileNotFoundError:
                raise FileNotFoundError(&#39;Files not found, make sure to run all processing functions.&#39;)

        # convert to SLE
        self.data[&#39;sle&#39;] = self.data.ivaf / 1e9 / 362.5  # m^3 / 1e9 / 362.5 = Gt / 1 mm --&gt; mm SLE
        self.data[&#39;modelname&#39;] = self.data.groupname + &#39;_&#39; + self.data.modelname


        # Save data on initial conditions for use in data splitting
        unique_batches = self.data.groupby([&#39;modelname&#39;, &#39;sectors&#39;, &#39;exp_id&#39;]).size().reset_index().rename(
            columns={0: &#39;count&#39;}).drop(columns=&#39;count&#39;)
        self.batches = unique_batches.values.tolist()
        self.output_columns = [&#39;icearea&#39;, &#39;iareafl&#39;, &#39;iareagr&#39;, &#39;ivol&#39;, &#39;ivaf&#39;, &#39;smb&#39;, &#39;smbgr&#39;, &#39;bmbfl&#39;, &#39;sle&#39;]

        for col in [&#39;icearea&#39;, &#39;iareafl&#39;, &#39;iareagr&#39;, &#39;ivol&#39;, &#39;smb&#39;, &#39;smbgr&#39;, &#39;bmbfl&#39;, &#39;sle&#39;]:
            self.data[col] = self.data[col].fillna(self.data[col].mean())

        self.X = None
        self.y = None
        self.scaler_X = None
        self.scaler_y = None

    def process(self, target_column: str=&#39;sle&#39;, drop_missing: bool=True, 
                drop_columns: list[str]=True, boolean_indices: bool=True, scale: bool=True,
                split_type: str=&#39;batch&#39;, drop_outliers: str=False, drop_expression: list[tuple]=None,
                time_series: bool=False, lag: int=None):
        &#34;&#34;&#34;Carries out feature engineering &amp; processing of formatted data. Includes dropping missing
        values, eliminating columns, creating boolean indices of categorical variables, scaling, 
        dataset splitting, and more. Refer to the individual functions contained in the source
        code for more information on each process.

        Args:
            target_column (str, optional): Column to be predicted. Defaults to &#39;sle&#39;.
            drop_missing (bool, optional): Flag denoting whether to drop missing values. Defaults to True.
            drop_columns (list[str], optional): List containing which columns (variables) to be dropped. Should be List[str] or boolean. If True is chosen, columns are dropped that will result in optimal performance. Defaults to True.
            boolean_indices (bool, optional): Flag denoting whether to create boolean indices for all categorical variables left after dropping columns. Defaults to True.
            scale (bool, optional): Flag denoting whether to scale data between zero and 1.  Sklearn&#39;s MinMaxScaler is used. Defaults to True.
            split_type (str, optional): Method to split data into training and testing set, must be in [random, batch]. Random is not recommended but is included for completeness. Defaults to &#39;batch&#39;.
            drop_outliers (str, optional): Method by which outliers will be dropped, must be in [quantile, explicit]. Defaults to False.
            drop_expression (list[tuple], optional): Expressions by which to drop outliers, see EmulatorData.drop_outliers. If drop_outliers==quantile, drop_expression must be a list[float] containing quantile bounds. Defaults to None. 
            time_series (bool, optional): Flag denoting whether to process the data as a time-series dataset or traditional non-time dataset. Defaults to False.
            lag (int, optional): Lag variable for time-series processing. Defaults to None.
            
        Returns:
            tuple: Multi-output returning [EmulatorData, train_features, test_features, train_labels, test_labels]
        &#34;&#34;&#34;


        if time_series:
            if lag is None:
                raise ValueError(&#39;If time_series == True, lag cannot be None&#39;)
            time_dependent_columns = [&#39;salinity&#39;, &#39;temperature&#39;, &#39;thermal_forcing&#39;,
                                      &#39;pr_anomaly&#39;, &#39;evspsbl_anomaly&#39;, &#39;mrro_anomaly&#39;, &#39;smb_anomaly&#39;,
                                      &#39;ts_anomaly&#39;,]
            separated_dfs = [y for x, y in self.data.groupby([&#39;sectors&#39;, &#39;exp_id&#39;, &#39;modelname&#39;])]
            for df in separated_dfs:
                for shift in range(1, lag + 1):
                    for column in time_dependent_columns:
                        df[f&#34;{column}.lag{shift}&#34;] = df[column].shift(shift, fill_value=0)
            self.data = pd.concat(separated_dfs)


        if drop_columns:
            if drop_columns is True:
                self.drop_columns(columns=[&#39;experiment&#39;, &#39;exp_id&#39;, &#39;groupname&#39;, &#39;regions&#39;])
            elif isinstance(drop_columns, list):
                self.drop_columns(columns=drop_columns)
            else:
                raise ValueError(f&#39;drop_columns argument must be of type boolean|list, received {type(drop_columns)}&#39;)

        if drop_missing:
            self = self.drop_missing()

        if boolean_indices:
            self = self.create_boolean_indices()

        if drop_outliers:
            if drop_outliers.lower() == &#39;quantile&#39;:
                self = self.drop_outliers(method=drop_outliers, quantiles=drop_expression)
            elif drop_outliers.lower() == &#39;explicit&#39;:
                self = self.drop_outliers(method=drop_outliers, expression=drop_expression)
            else:
                raise ValueError(&#39;drop_outliers argument must be in [quantile, explicit]&#39;)
            
            

        self = self.split_data(target_column=target_column)

        if scale:
            self.X = self.scale(self.X, &#39;inputs&#39;, scaler=&#39;MinMaxScaler&#39;)
            if target_column == &#39;sle&#39;:
                self.y = np.array(self.y)
            else:
                self.y = self.scale(self.y, &#39;outputs&#39;, scaler=&#39;MinMaxScaler&#39;)

        self = self.train_test_split(split_type=split_type)

        return self, self.train_features, self.test_features, self.train_labels, self.test_labels

    def drop_outliers(self, method: str, expression: list[tuple]=None, quantiles: list[float]=[0.01, 0.99]):
        &#34;&#34;&#34;Drops simulations that are outliers based on the provided method and expression. 
        Extra complexity is handled due to the necessity of removing the entire 85 row series from 
        the dataset rather than simply removing the rows with given conditions. Note that the 
        condition indicates rows to be DROPPED, not kept (e.g. &#39;sle&#39;, &#39;&gt;&#39;, &#39;20&#39; would drop all
        simulations containing sle values over 20). If quantile method is used, outliers are dropped
        from the SLE column based on the provided quantile in the quantiles argument. If explicit is
        chosen, expression must contain a list of tuples such that the tuple contains 
        [(column, operator, expression)] of the subset, e.g. [(&#34;sle&#34;, &#34;&gt;&#34;, 20), (&#34;sle&#34;, &#34;&lt;&#34;, -20)].

        Args:
            method (str): Method of outlier deletion, must be in [quantile, explicit]
            expression (list[tuple]): List of subset expressions in the form [column, operator, value], defaults to None.
            quantiles (list[float]): , defaults to [0.01, 0.99].

        Returns:
            EmulatorData: self, with self.data having outliers dropped.
        &#34;&#34;&#34;
        
        if method.lower() == &#39;quantile&#39;:
            if quantiles is None:
                raise AttributeError(&#39;If method == quantile, quantiles argument cannot be None&#39;)
            lower_sle, upper_sle = np.quantile(np.array(self.data.sle), quantiles)
            outlier_data = self.data[(self.data[&#39;sle&#39;] &lt;= lower_sle) | (self.data[&#39;sle&#39;] &gt;= upper_sle)]
        elif method.lower() == &#39;explicit&#39;:
            
            if expression is None:
                raise AttributeError(&#39;If method == explicit, expression argument cannot be None&#39;)
            elif not isinstance(expression, list) or not isinstance(expression[0], tuple):
                raise AttributeError(&#39;Expression argument must be a list of tuples, e.g. [(&#34;sle&#34;, &#34;&gt;&#34;, 20), (&#34;sle&#34;, &#34;&lt;&#34;, -20)]&#39;)
            
            outlier_data = self.data
            for subset_expression in expression:
                column, operator, value = subset_expression            

                if operator.lower() in (&#39;equal&#39;, &#39;equals&#39;, &#39;=&#39;, &#39;==&#39;):
                    outlier_data = outlier_data[outlier_data[column] == value]
                elif operator.lower() in (&#39;not equal&#39;, &#39;not equals&#39;, &#39;!=&#39;, &#39;~=&#39;):
                    outlier_data = outlier_data[outlier_data[column] != value]
                elif operator.lower() in (&#39;greather than&#39;, &#39;greater&#39;, &#39;&gt;=&#39;, &#39;&gt;&#39;):
                    outlier_data = outlier_data[outlier_data[column] &gt; value]
                elif operator.lower() in (&#39;less than&#39;, &#39;less&#39;, &#39;&lt;=&#39;, &#39;&lt;&#39;):
                    outlier_data = outlier_data[outlier_data[column] &lt; value]
                else:
                    raise ValueError(f&#39;Operator must be in [\&#34;==\&#34;, \&#34;!=\&#34;, \&#34;&gt;\&#34;, \&#34;&lt;\&#34;], received {operator}&#39;)

        cols = outlier_data.columns
        nonzero_columns = outlier_data.apply(lambda x: x &gt; 0).apply(lambda x: list(cols[x.values]), axis=1)

        # Create dataframe of experiments with outliers (want to delete the entire 85 rows)
        outlier_runs = pd.DataFrame()
        outlier_runs[&#39;modelname&#39;] = nonzero_columns.apply(lambda x: x[-6])
        outlier_runs[&#39;exp_id&#39;] = nonzero_columns.apply(lambda x: x[-5])
        outlier_runs[&#39;sectors&#39;] = outlier_data.sectors
        outlier_runs_list = outlier_runs.values.tolist()
        unique_outliers = [list(x) for x in set(tuple(x) for x in outlier_runs_list)]

        # Drop those runs
        for i in unique_outliers:
            modelname = i[0]
            exp_id = i[1]
            sector = i[2]
            self.data = self.data.drop(
                self.data[(self.data[modelname] == 1) &amp; (self.data[exp_id] == 1) &amp; (self.data.sectors == sector)].index
            )

        return self

    def split_data(self, target_column: str, ):
        &#34;&#34;&#34;Splits data into features and labels based on target column.

        Args:
            target_column (str): Output column to be predicted.

        Returns:
            EmulatorData: self, with self.X and self.y as attributes.
        &#34;&#34;&#34;
        self.target_column = target_column
        self.X = self.data.drop(columns=self.output_columns)
        self.y = self.data[target_column]
        self.input_columns = self.X.columns
        return self

    def train_test_split(self, train_size: float=0.7, split_type: str=&#39;batch&#39;):
        &#34;&#34;&#34;Splits dataset into training set and testing set. Can be split using two different
        methods: random and batch. The random method splits by randomly sampling rows, whereas 
        batch method randomly samples entire simulation series (85 rows) in order to keep simulations
        together during testing. Random method is included for completeness but is not recommended
        for use in emulator creation.

        Args:
            train_size (float, optional): Proportion of data in training set, between 0 and 1. Defaults to 0.7.
            split_type (str, optional): Splitting method, must be in [random, batch]. Defaults to &#39;batch&#39;.

        Returns:
            EmulatorData: self, with self.train_features, self.test_features, self.train_labels, self.test_labels as attributes.
        &#34;&#34;&#34;

        if not isinstance(self.X, pd.DataFrame):
            self.X = pd.DataFrame(self.X, columns=self.input_columns)

        if &#39;random&#39; in split_type.lower():
            self.train_features = self.X.sample(frac=train_size, random_state=0)
            training_indices = self.train_features.index
            self.train_labels = self.y[training_indices].squeeze()

            self.test_features = self.X.drop(training_indices)
            self.test_labels = pd.Series(self.y.squeeze()).drop(training_indices)


        elif split_type.lower() == &#34;batch&#34;:
            # batch -- grouping of 85 years of a particular model, experiment, and sector
            # Calculate how many batches you&#39;ll need (roughly) for train/test proportion
            test_num_rows = len(self.X) * (1 - train_size)
            num_years = len(set(self.data.year))
            num_test_batches = test_num_rows // num_years

            # Get all possible values for sector, experiment, and model
            all_sectors = list(set(self.X.sectors))
            all_experiments = [col for col in self.X.columns if &#34;exp_id&#34; in col]
            all_modelnames = [col for col in self.X.columns if &#34;modelname&#34; in col]

            # Set up concatenation of test data scenarios...
            test_scenarios = []
            test_dataset = pd.DataFrame()

            # Keep this running until you have enough samples
            np.random.seed(10)
            while len(test_scenarios) &lt; num_test_batches:
                # Get a random
                random_model = np.random.choice(all_modelnames)
                random_sector = np.random.choice(all_sectors)
                random_experiment = np.random.choice(all_experiments)
                test_scenario = [random_model, random_sector, random_experiment]
                if test_scenario not in test_scenarios:
                    scenario_df = self.X[(self.X[random_model] == 1) &amp; (self.X[&#39;sectors&#39;] == random_sector) &amp; (
                            self.X[random_experiment] == 1)]
                    if not scenario_df.empty:
                        test_scenarios.append(test_scenario)
                        test_dataset = pd.concat([test_dataset, scenario_df])
            self.test_features = test_dataset
            testing_indices = self.test_features.index
            self.test_labels = self.y[testing_indices].squeeze()

            self.train_features = self.X.drop(testing_indices)
            self.train_labels = pd.Series(self.y.squeeze()).drop(testing_indices)

            self.test_scenarios = test_scenarios
        
        else:
            raise(f&#39;split_type must be in [random, batch], received {split_type}&#39;)

        return self

    def drop_missing(self):
        &#34;&#34;&#34;Drops rows with missing values (wrapper for pandas.DataFrame.dropna()).

        Returns:
            EmulatorData: self, with NA values dropped from self.data.
        &#34;&#34;&#34;
        self.data = self.data.dropna()
        return self

    def create_boolean_indices(self, columns: str=&#39;all&#39;):
        &#34;&#34;&#34;Creates boolean indices (one hot encoding) for categoritcal variables in columns 
        argument. Wrapper for pandas.get_dummies() with added functionality for prefix separation.

        Args:
            columns (str, optional): Categorical variables to be encoded. Defaults to &#39;all&#39;.

        Returns:
            EmulatorData: self, with boolean indices in self.data.
        &#34;&#34;&#34;
        if columns == &#39;all&#39;:
            self.data = pd.get_dummies(self.data, prefix_sep=&#34;-&#34;)
        else:
            if not isinstance(columns, list):
                raise ValueError(f&#39;Columns argument must be of type: list, received {type(columns)}.&#39;)

            self.data = pd.get_dummies(self.data, columns=columns, prefix_sep=&#34;-&#34;)

            for col in self.data.columns:
                self.data[col] = self.data[col].astype(float)
        return self

    def drop_columns(self, columns: list[str]):
        &#34;&#34;&#34;Drops columns in columns argument from the dataset. Wrapper for pandas.DataFrame.drop()
        with error checking.

        Args:
            columns (list[str]): List of columns (or singular string column) to be dropped from the dataset.

        Returns:
            EmulatorData: self, with desired columns dropped from self.data.
        &#34;&#34;&#34;
        if not isinstance(columns, list) and not isinstance(columns, str):
            raise ValueError(f&#39;Columns argument must be of type: str|list, received {type(columns)}.&#39;)
        columns = list(columns)

        self.data = self.data.drop(columns=columns)

        return self

    def scale(self, values: pd.DataFrame, values_type: str, scaler: str=&#34;MinMaxScaler&#34;):
        &#34;&#34;&#34;Scales dataframe and saves scaler for future use in unscaling. Sklearn&#39;s scaling API is
        used. MinMaxScaler is recommended but StandardScaler is also supported.

        Args:
            values (pd.DataFrame): Dataframe to be scaled.
            values_type (str): Whether the dataframe to be scaled is a feature or labels dataframe, must be in [inputs, outputs]
            scaler (str, optional): Type of scaler to be used, must be in [MinMaxScaler, StandardScaler]. Defaults to &#34;MinMaxScaler&#34;.

        Returns:
            pd.DataFrame: scaled dataset with self.scaler_X and self.scaler_y as attributes in the EmulatorData class.
        &#34;&#34;&#34;
        if self.X is None and self.y is None:
            raise AttributeError(&#39;Data must be split before scaling using model.split_data method.&#39;)

        if &#34;minmax&#34; in scaler.lower():
            if &#39;input&#39; in values_type.lower():
                self.scaler_X = sp.MinMaxScaler()
            else:
                self.scaler_y = sp.MinMaxScaler()
        elif &#34;standard&#34; in scaler.lower():
            if &#39;input&#39; in values_type.lower():
                self.scaler_X = sp.StandardScaler()
            else:
                self.scaler_y = sp.StandardScaler()
        else:
            raise ValueError(f&#39;scaler argument must be in [\&#39;MinMaxScaler\&#39;, \&#39;StandardScaler\&#39;], received {scaler}&#39;)

        if &#39;input&#39; in values_type.lower():
            self.input_columns = self.X.columns
            self.scaler_X.fit(self.X)
            return pd.DataFrame(self.scaler_X.transform(values), columns=self.X.columns)

        # TODO: Don&#39;t need this anymore with SLE as the prediction
        elif &#39;output&#39; in values_type.lower():
            self.scaler_y.fit(np.array(self.y).reshape(-1, 1))
            return self.scaler_y.transform(np.array(values).reshape(-1, 1))

        else:
            raise ValueError(f&#34;values_type must be in [&#39;inputs&#39;, &#39;outputs&#39;], received {values_type}&#34;)

    def unscale(self, values: pd.DataFrame, values_type: str):
        &#34;&#34;&#34;Unscales data based on scalers trained in EmulatorData.scale().

        Args:
            values (pd.DataFrame): Dataframe to be unscaled.
            values_type (str): Whether the dataframe to be unscaled is a feature or labels dataframe, must be in [inputs, outputs]

        Returns:
           pd.DataFrame: unscaled dataset.
        &#34;&#34;&#34;

        if &#39;input&#39; in values_type.lower():
            return pd.DataFrame(self.scaler_X.inverse_transform(values), columns=self.input_columns)

        elif &#39;output&#39; in values_type.lower():
            return self.scaler_y.inverse_transform(values.reshape(-1, 1))

        else:
            raise ValueError(f&#34;values_type must be in [&#39;inputs&#39;, &#39;outputs&#39;], received {values_type}&#34;)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="ise.data.EmulatorData.EmulatorData.create_boolean_indices"><code class="name flex">
<span>def <span class="ident">create_boolean_indices</span></span>(<span>self, columns: str = 'all')</span>
</code></dt>
<dd>
<div class="desc"><p>Creates boolean indices (one hot encoding) for categoritcal variables in columns
argument. Wrapper for pandas.get_dummies() with added functionality for prefix separation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>columns</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Categorical variables to be encoded. Defaults to 'all'.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="ise.data.EmulatorData.EmulatorData" href="#ise.data.EmulatorData.EmulatorData">EmulatorData</a></code></dt>
<dd>self, with boolean indices in self.data.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_boolean_indices(self, columns: str=&#39;all&#39;):
    &#34;&#34;&#34;Creates boolean indices (one hot encoding) for categoritcal variables in columns 
    argument. Wrapper for pandas.get_dummies() with added functionality for prefix separation.

    Args:
        columns (str, optional): Categorical variables to be encoded. Defaults to &#39;all&#39;.

    Returns:
        EmulatorData: self, with boolean indices in self.data.
    &#34;&#34;&#34;
    if columns == &#39;all&#39;:
        self.data = pd.get_dummies(self.data, prefix_sep=&#34;-&#34;)
    else:
        if not isinstance(columns, list):
            raise ValueError(f&#39;Columns argument must be of type: list, received {type(columns)}.&#39;)

        self.data = pd.get_dummies(self.data, columns=columns, prefix_sep=&#34;-&#34;)

        for col in self.data.columns:
            self.data[col] = self.data[col].astype(float)
    return self</code></pre>
</details>
</dd>
<dt id="ise.data.EmulatorData.EmulatorData.drop_columns"><code class="name flex">
<span>def <span class="ident">drop_columns</span></span>(<span>self, columns: list[str])</span>
</code></dt>
<dd>
<div class="desc"><p>Drops columns in columns argument from the dataset. Wrapper for pandas.DataFrame.drop()
with error checking.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>columns</code></strong> :&ensp;<code>list[str]</code></dt>
<dd>List of columns (or singular string column) to be dropped from the dataset.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="ise.data.EmulatorData.EmulatorData" href="#ise.data.EmulatorData.EmulatorData">EmulatorData</a></code></dt>
<dd>self, with desired columns dropped from self.data.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def drop_columns(self, columns: list[str]):
    &#34;&#34;&#34;Drops columns in columns argument from the dataset. Wrapper for pandas.DataFrame.drop()
    with error checking.

    Args:
        columns (list[str]): List of columns (or singular string column) to be dropped from the dataset.

    Returns:
        EmulatorData: self, with desired columns dropped from self.data.
    &#34;&#34;&#34;
    if not isinstance(columns, list) and not isinstance(columns, str):
        raise ValueError(f&#39;Columns argument must be of type: str|list, received {type(columns)}.&#39;)
    columns = list(columns)

    self.data = self.data.drop(columns=columns)

    return self</code></pre>
</details>
</dd>
<dt id="ise.data.EmulatorData.EmulatorData.drop_missing"><code class="name flex">
<span>def <span class="ident">drop_missing</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Drops rows with missing values (wrapper for pandas.DataFrame.dropna()).</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="ise.data.EmulatorData.EmulatorData" href="#ise.data.EmulatorData.EmulatorData">EmulatorData</a></code></dt>
<dd>self, with NA values dropped from self.data.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def drop_missing(self):
    &#34;&#34;&#34;Drops rows with missing values (wrapper for pandas.DataFrame.dropna()).

    Returns:
        EmulatorData: self, with NA values dropped from self.data.
    &#34;&#34;&#34;
    self.data = self.data.dropna()
    return self</code></pre>
</details>
</dd>
<dt id="ise.data.EmulatorData.EmulatorData.drop_outliers"><code class="name flex">
<span>def <span class="ident">drop_outliers</span></span>(<span>self, method: str, expression: list[tuple] = None, quantiles: list[float] = [0.01, 0.99])</span>
</code></dt>
<dd>
<div class="desc"><p>Drops simulations that are outliers based on the provided method and expression.
Extra complexity is handled due to the necessity of removing the entire 85 row series from
the dataset rather than simply removing the rows with given conditions. Note that the
condition indicates rows to be DROPPED, not kept (e.g. 'sle', '&gt;', '20' would drop all
simulations containing sle values over 20). If quantile method is used, outliers are dropped
from the SLE column based on the provided quantile in the quantiles argument. If explicit is
chosen, expression must contain a list of tuples such that the tuple contains
[(column, operator, expression)] of the subset, e.g. [("sle", "&gt;", 20), ("sle", "&lt;", -20)].</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>method</code></strong> :&ensp;<code>str</code></dt>
<dd>Method of outlier deletion, must be in [quantile, explicit]</dd>
<dt><strong><code>expression</code></strong> :&ensp;<code>list[tuple]</code></dt>
<dd>List of subset expressions in the form [column, operator, value], defaults to None.</dd>
<dt><strong><code>quantiles</code></strong> :&ensp;<code>list[float]</code></dt>
<dd>, defaults to [0.01, 0.99].</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="ise.data.EmulatorData.EmulatorData" href="#ise.data.EmulatorData.EmulatorData">EmulatorData</a></code></dt>
<dd>self, with self.data having outliers dropped.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def drop_outliers(self, method: str, expression: list[tuple]=None, quantiles: list[float]=[0.01, 0.99]):
    &#34;&#34;&#34;Drops simulations that are outliers based on the provided method and expression. 
    Extra complexity is handled due to the necessity of removing the entire 85 row series from 
    the dataset rather than simply removing the rows with given conditions. Note that the 
    condition indicates rows to be DROPPED, not kept (e.g. &#39;sle&#39;, &#39;&gt;&#39;, &#39;20&#39; would drop all
    simulations containing sle values over 20). If quantile method is used, outliers are dropped
    from the SLE column based on the provided quantile in the quantiles argument. If explicit is
    chosen, expression must contain a list of tuples such that the tuple contains 
    [(column, operator, expression)] of the subset, e.g. [(&#34;sle&#34;, &#34;&gt;&#34;, 20), (&#34;sle&#34;, &#34;&lt;&#34;, -20)].

    Args:
        method (str): Method of outlier deletion, must be in [quantile, explicit]
        expression (list[tuple]): List of subset expressions in the form [column, operator, value], defaults to None.
        quantiles (list[float]): , defaults to [0.01, 0.99].

    Returns:
        EmulatorData: self, with self.data having outliers dropped.
    &#34;&#34;&#34;
    
    if method.lower() == &#39;quantile&#39;:
        if quantiles is None:
            raise AttributeError(&#39;If method == quantile, quantiles argument cannot be None&#39;)
        lower_sle, upper_sle = np.quantile(np.array(self.data.sle), quantiles)
        outlier_data = self.data[(self.data[&#39;sle&#39;] &lt;= lower_sle) | (self.data[&#39;sle&#39;] &gt;= upper_sle)]
    elif method.lower() == &#39;explicit&#39;:
        
        if expression is None:
            raise AttributeError(&#39;If method == explicit, expression argument cannot be None&#39;)
        elif not isinstance(expression, list) or not isinstance(expression[0], tuple):
            raise AttributeError(&#39;Expression argument must be a list of tuples, e.g. [(&#34;sle&#34;, &#34;&gt;&#34;, 20), (&#34;sle&#34;, &#34;&lt;&#34;, -20)]&#39;)
        
        outlier_data = self.data
        for subset_expression in expression:
            column, operator, value = subset_expression            

            if operator.lower() in (&#39;equal&#39;, &#39;equals&#39;, &#39;=&#39;, &#39;==&#39;):
                outlier_data = outlier_data[outlier_data[column] == value]
            elif operator.lower() in (&#39;not equal&#39;, &#39;not equals&#39;, &#39;!=&#39;, &#39;~=&#39;):
                outlier_data = outlier_data[outlier_data[column] != value]
            elif operator.lower() in (&#39;greather than&#39;, &#39;greater&#39;, &#39;&gt;=&#39;, &#39;&gt;&#39;):
                outlier_data = outlier_data[outlier_data[column] &gt; value]
            elif operator.lower() in (&#39;less than&#39;, &#39;less&#39;, &#39;&lt;=&#39;, &#39;&lt;&#39;):
                outlier_data = outlier_data[outlier_data[column] &lt; value]
            else:
                raise ValueError(f&#39;Operator must be in [\&#34;==\&#34;, \&#34;!=\&#34;, \&#34;&gt;\&#34;, \&#34;&lt;\&#34;], received {operator}&#39;)

    cols = outlier_data.columns
    nonzero_columns = outlier_data.apply(lambda x: x &gt; 0).apply(lambda x: list(cols[x.values]), axis=1)

    # Create dataframe of experiments with outliers (want to delete the entire 85 rows)
    outlier_runs = pd.DataFrame()
    outlier_runs[&#39;modelname&#39;] = nonzero_columns.apply(lambda x: x[-6])
    outlier_runs[&#39;exp_id&#39;] = nonzero_columns.apply(lambda x: x[-5])
    outlier_runs[&#39;sectors&#39;] = outlier_data.sectors
    outlier_runs_list = outlier_runs.values.tolist()
    unique_outliers = [list(x) for x in set(tuple(x) for x in outlier_runs_list)]

    # Drop those runs
    for i in unique_outliers:
        modelname = i[0]
        exp_id = i[1]
        sector = i[2]
        self.data = self.data.drop(
            self.data[(self.data[modelname] == 1) &amp; (self.data[exp_id] == 1) &amp; (self.data.sectors == sector)].index
        )

    return self</code></pre>
</details>
</dd>
<dt id="ise.data.EmulatorData.EmulatorData.process"><code class="name flex">
<span>def <span class="ident">process</span></span>(<span>self, target_column: str = 'sle', drop_missing: bool = True, drop_columns: list[str] = True, boolean_indices: bool = True, scale: bool = True, split_type: str = 'batch', drop_outliers: str = False, drop_expression: list[tuple] = None, time_series: bool = False, lag: int = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Carries out feature engineering &amp; processing of formatted data. Includes dropping missing
values, eliminating columns, creating boolean indices of categorical variables, scaling,
dataset splitting, and more. Refer to the individual functions contained in the source
code for more information on each process.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>target_column</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Column to be predicted. Defaults to 'sle'.</dd>
<dt><strong><code>drop_missing</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Flag denoting whether to drop missing values. Defaults to True.</dd>
<dt><strong><code>drop_columns</code></strong> :&ensp;<code>list[str]</code>, optional</dt>
<dd>List containing which columns (variables) to be dropped. Should be List[str] or boolean. If True is chosen, columns are dropped that will result in optimal performance. Defaults to True.</dd>
<dt><strong><code>boolean_indices</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Flag denoting whether to create boolean indices for all categorical variables left after dropping columns. Defaults to True.</dd>
<dt><strong><code>scale</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Flag denoting whether to scale data between zero and 1.
Sklearn's MinMaxScaler is used. Defaults to True.</dd>
<dt><strong><code>split_type</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Method to split data into training and testing set, must be in [random, batch]. Random is not recommended but is included for completeness. Defaults to 'batch'.</dd>
<dt><strong><code>drop_outliers</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Method by which outliers will be dropped, must be in [quantile, explicit]. Defaults to False.</dd>
<dt><strong><code>drop_expression</code></strong> :&ensp;<code>list[tuple]</code>, optional</dt>
<dd>Expressions by which to drop outliers, see EmulatorData.drop_outliers. If drop_outliers==quantile, drop_expression must be a list[float] containing quantile bounds. Defaults to None. </dd>
<dt><strong><code>time_series</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Flag denoting whether to process the data as a time-series dataset or traditional non-time dataset. Defaults to False.</dd>
<dt><strong><code>lag</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Lag variable for time-series processing. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>Multi-output returning [EmulatorData, train_features, test_features, train_labels, test_labels]</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process(self, target_column: str=&#39;sle&#39;, drop_missing: bool=True, 
            drop_columns: list[str]=True, boolean_indices: bool=True, scale: bool=True,
            split_type: str=&#39;batch&#39;, drop_outliers: str=False, drop_expression: list[tuple]=None,
            time_series: bool=False, lag: int=None):
    &#34;&#34;&#34;Carries out feature engineering &amp; processing of formatted data. Includes dropping missing
    values, eliminating columns, creating boolean indices of categorical variables, scaling, 
    dataset splitting, and more. Refer to the individual functions contained in the source
    code for more information on each process.

    Args:
        target_column (str, optional): Column to be predicted. Defaults to &#39;sle&#39;.
        drop_missing (bool, optional): Flag denoting whether to drop missing values. Defaults to True.
        drop_columns (list[str], optional): List containing which columns (variables) to be dropped. Should be List[str] or boolean. If True is chosen, columns are dropped that will result in optimal performance. Defaults to True.
        boolean_indices (bool, optional): Flag denoting whether to create boolean indices for all categorical variables left after dropping columns. Defaults to True.
        scale (bool, optional): Flag denoting whether to scale data between zero and 1.  Sklearn&#39;s MinMaxScaler is used. Defaults to True.
        split_type (str, optional): Method to split data into training and testing set, must be in [random, batch]. Random is not recommended but is included for completeness. Defaults to &#39;batch&#39;.
        drop_outliers (str, optional): Method by which outliers will be dropped, must be in [quantile, explicit]. Defaults to False.
        drop_expression (list[tuple], optional): Expressions by which to drop outliers, see EmulatorData.drop_outliers. If drop_outliers==quantile, drop_expression must be a list[float] containing quantile bounds. Defaults to None. 
        time_series (bool, optional): Flag denoting whether to process the data as a time-series dataset or traditional non-time dataset. Defaults to False.
        lag (int, optional): Lag variable for time-series processing. Defaults to None.
        
    Returns:
        tuple: Multi-output returning [EmulatorData, train_features, test_features, train_labels, test_labels]
    &#34;&#34;&#34;


    if time_series:
        if lag is None:
            raise ValueError(&#39;If time_series == True, lag cannot be None&#39;)
        time_dependent_columns = [&#39;salinity&#39;, &#39;temperature&#39;, &#39;thermal_forcing&#39;,
                                  &#39;pr_anomaly&#39;, &#39;evspsbl_anomaly&#39;, &#39;mrro_anomaly&#39;, &#39;smb_anomaly&#39;,
                                  &#39;ts_anomaly&#39;,]
        separated_dfs = [y for x, y in self.data.groupby([&#39;sectors&#39;, &#39;exp_id&#39;, &#39;modelname&#39;])]
        for df in separated_dfs:
            for shift in range(1, lag + 1):
                for column in time_dependent_columns:
                    df[f&#34;{column}.lag{shift}&#34;] = df[column].shift(shift, fill_value=0)
        self.data = pd.concat(separated_dfs)


    if drop_columns:
        if drop_columns is True:
            self.drop_columns(columns=[&#39;experiment&#39;, &#39;exp_id&#39;, &#39;groupname&#39;, &#39;regions&#39;])
        elif isinstance(drop_columns, list):
            self.drop_columns(columns=drop_columns)
        else:
            raise ValueError(f&#39;drop_columns argument must be of type boolean|list, received {type(drop_columns)}&#39;)

    if drop_missing:
        self = self.drop_missing()

    if boolean_indices:
        self = self.create_boolean_indices()

    if drop_outliers:
        if drop_outliers.lower() == &#39;quantile&#39;:
            self = self.drop_outliers(method=drop_outliers, quantiles=drop_expression)
        elif drop_outliers.lower() == &#39;explicit&#39;:
            self = self.drop_outliers(method=drop_outliers, expression=drop_expression)
        else:
            raise ValueError(&#39;drop_outliers argument must be in [quantile, explicit]&#39;)
        
        

    self = self.split_data(target_column=target_column)

    if scale:
        self.X = self.scale(self.X, &#39;inputs&#39;, scaler=&#39;MinMaxScaler&#39;)
        if target_column == &#39;sle&#39;:
            self.y = np.array(self.y)
        else:
            self.y = self.scale(self.y, &#39;outputs&#39;, scaler=&#39;MinMaxScaler&#39;)

    self = self.train_test_split(split_type=split_type)

    return self, self.train_features, self.test_features, self.train_labels, self.test_labels</code></pre>
</details>
</dd>
<dt id="ise.data.EmulatorData.EmulatorData.scale"><code class="name flex">
<span>def <span class="ident">scale</span></span>(<span>self, values: pandas.core.frame.DataFrame, values_type: str, scaler: str = 'MinMaxScaler')</span>
</code></dt>
<dd>
<div class="desc"><p>Scales dataframe and saves scaler for future use in unscaling. Sklearn's scaling API is
used. MinMaxScaler is recommended but StandardScaler is also supported.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Dataframe to be scaled.</dd>
<dt><strong><code>values_type</code></strong> :&ensp;<code>str</code></dt>
<dd>Whether the dataframe to be scaled is a feature or labels dataframe, must be in [inputs, outputs]</dd>
<dt><strong><code>scaler</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Type of scaler to be used, must be in [MinMaxScaler, StandardScaler]. Defaults to "MinMaxScaler".</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>scaled dataset with self.scaler_X and self.scaler_y as attributes in the EmulatorData class.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def scale(self, values: pd.DataFrame, values_type: str, scaler: str=&#34;MinMaxScaler&#34;):
    &#34;&#34;&#34;Scales dataframe and saves scaler for future use in unscaling. Sklearn&#39;s scaling API is
    used. MinMaxScaler is recommended but StandardScaler is also supported.

    Args:
        values (pd.DataFrame): Dataframe to be scaled.
        values_type (str): Whether the dataframe to be scaled is a feature or labels dataframe, must be in [inputs, outputs]
        scaler (str, optional): Type of scaler to be used, must be in [MinMaxScaler, StandardScaler]. Defaults to &#34;MinMaxScaler&#34;.

    Returns:
        pd.DataFrame: scaled dataset with self.scaler_X and self.scaler_y as attributes in the EmulatorData class.
    &#34;&#34;&#34;
    if self.X is None and self.y is None:
        raise AttributeError(&#39;Data must be split before scaling using model.split_data method.&#39;)

    if &#34;minmax&#34; in scaler.lower():
        if &#39;input&#39; in values_type.lower():
            self.scaler_X = sp.MinMaxScaler()
        else:
            self.scaler_y = sp.MinMaxScaler()
    elif &#34;standard&#34; in scaler.lower():
        if &#39;input&#39; in values_type.lower():
            self.scaler_X = sp.StandardScaler()
        else:
            self.scaler_y = sp.StandardScaler()
    else:
        raise ValueError(f&#39;scaler argument must be in [\&#39;MinMaxScaler\&#39;, \&#39;StandardScaler\&#39;], received {scaler}&#39;)

    if &#39;input&#39; in values_type.lower():
        self.input_columns = self.X.columns
        self.scaler_X.fit(self.X)
        return pd.DataFrame(self.scaler_X.transform(values), columns=self.X.columns)

    # TODO: Don&#39;t need this anymore with SLE as the prediction
    elif &#39;output&#39; in values_type.lower():
        self.scaler_y.fit(np.array(self.y).reshape(-1, 1))
        return self.scaler_y.transform(np.array(values).reshape(-1, 1))

    else:
        raise ValueError(f&#34;values_type must be in [&#39;inputs&#39;, &#39;outputs&#39;], received {values_type}&#34;)</code></pre>
</details>
</dd>
<dt id="ise.data.EmulatorData.EmulatorData.split_data"><code class="name flex">
<span>def <span class="ident">split_data</span></span>(<span>self, target_column: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Splits data into features and labels based on target column.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>target_column</code></strong> :&ensp;<code>str</code></dt>
<dd>Output column to be predicted.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="ise.data.EmulatorData.EmulatorData" href="#ise.data.EmulatorData.EmulatorData">EmulatorData</a></code></dt>
<dd>self, with self.X and self.y as attributes.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def split_data(self, target_column: str, ):
    &#34;&#34;&#34;Splits data into features and labels based on target column.

    Args:
        target_column (str): Output column to be predicted.

    Returns:
        EmulatorData: self, with self.X and self.y as attributes.
    &#34;&#34;&#34;
    self.target_column = target_column
    self.X = self.data.drop(columns=self.output_columns)
    self.y = self.data[target_column]
    self.input_columns = self.X.columns
    return self</code></pre>
</details>
</dd>
<dt id="ise.data.EmulatorData.EmulatorData.train_test_split"><code class="name flex">
<span>def <span class="ident">train_test_split</span></span>(<span>self, train_size: float = 0.7, split_type: str = 'batch')</span>
</code></dt>
<dd>
<div class="desc"><p>Splits dataset into training set and testing set. Can be split using two different
methods: random and batch. The random method splits by randomly sampling rows, whereas
batch method randomly samples entire simulation series (85 rows) in order to keep simulations
together during testing. Random method is included for completeness but is not recommended
for use in emulator creation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>train_size</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Proportion of data in training set, between 0 and 1. Defaults to 0.7.</dd>
<dt><strong><code>split_type</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Splitting method, must be in [random, batch]. Defaults to 'batch'.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="ise.data.EmulatorData.EmulatorData" href="#ise.data.EmulatorData.EmulatorData">EmulatorData</a></code></dt>
<dd>self, with self.train_features, self.test_features, self.train_labels, self.test_labels as attributes.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_test_split(self, train_size: float=0.7, split_type: str=&#39;batch&#39;):
    &#34;&#34;&#34;Splits dataset into training set and testing set. Can be split using two different
    methods: random and batch. The random method splits by randomly sampling rows, whereas 
    batch method randomly samples entire simulation series (85 rows) in order to keep simulations
    together during testing. Random method is included for completeness but is not recommended
    for use in emulator creation.

    Args:
        train_size (float, optional): Proportion of data in training set, between 0 and 1. Defaults to 0.7.
        split_type (str, optional): Splitting method, must be in [random, batch]. Defaults to &#39;batch&#39;.

    Returns:
        EmulatorData: self, with self.train_features, self.test_features, self.train_labels, self.test_labels as attributes.
    &#34;&#34;&#34;

    if not isinstance(self.X, pd.DataFrame):
        self.X = pd.DataFrame(self.X, columns=self.input_columns)

    if &#39;random&#39; in split_type.lower():
        self.train_features = self.X.sample(frac=train_size, random_state=0)
        training_indices = self.train_features.index
        self.train_labels = self.y[training_indices].squeeze()

        self.test_features = self.X.drop(training_indices)
        self.test_labels = pd.Series(self.y.squeeze()).drop(training_indices)


    elif split_type.lower() == &#34;batch&#34;:
        # batch -- grouping of 85 years of a particular model, experiment, and sector
        # Calculate how many batches you&#39;ll need (roughly) for train/test proportion
        test_num_rows = len(self.X) * (1 - train_size)
        num_years = len(set(self.data.year))
        num_test_batches = test_num_rows // num_years

        # Get all possible values for sector, experiment, and model
        all_sectors = list(set(self.X.sectors))
        all_experiments = [col for col in self.X.columns if &#34;exp_id&#34; in col]
        all_modelnames = [col for col in self.X.columns if &#34;modelname&#34; in col]

        # Set up concatenation of test data scenarios...
        test_scenarios = []
        test_dataset = pd.DataFrame()

        # Keep this running until you have enough samples
        np.random.seed(10)
        while len(test_scenarios) &lt; num_test_batches:
            # Get a random
            random_model = np.random.choice(all_modelnames)
            random_sector = np.random.choice(all_sectors)
            random_experiment = np.random.choice(all_experiments)
            test_scenario = [random_model, random_sector, random_experiment]
            if test_scenario not in test_scenarios:
                scenario_df = self.X[(self.X[random_model] == 1) &amp; (self.X[&#39;sectors&#39;] == random_sector) &amp; (
                        self.X[random_experiment] == 1)]
                if not scenario_df.empty:
                    test_scenarios.append(test_scenario)
                    test_dataset = pd.concat([test_dataset, scenario_df])
        self.test_features = test_dataset
        testing_indices = self.test_features.index
        self.test_labels = self.y[testing_indices].squeeze()

        self.train_features = self.X.drop(testing_indices)
        self.train_labels = pd.Series(self.y.squeeze()).drop(testing_indices)

        self.test_scenarios = test_scenarios
    
    else:
        raise(f&#39;split_type must be in [random, batch], received {split_type}&#39;)

    return self</code></pre>
</details>
</dd>
<dt id="ise.data.EmulatorData.EmulatorData.unscale"><code class="name flex">
<span>def <span class="ident">unscale</span></span>(<span>self, values: pandas.core.frame.DataFrame, values_type: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Unscales data based on scalers trained in EmulatorData.scale().</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Dataframe to be unscaled.</dd>
<dt><strong><code>values_type</code></strong> :&ensp;<code>str</code></dt>
<dd>Whether the dataframe to be unscaled is a feature or labels dataframe, must be in [inputs, outputs]</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>unscaled dataset.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unscale(self, values: pd.DataFrame, values_type: str):
    &#34;&#34;&#34;Unscales data based on scalers trained in EmulatorData.scale().

    Args:
        values (pd.DataFrame): Dataframe to be unscaled.
        values_type (str): Whether the dataframe to be unscaled is a feature or labels dataframe, must be in [inputs, outputs]

    Returns:
       pd.DataFrame: unscaled dataset.
    &#34;&#34;&#34;

    if &#39;input&#39; in values_type.lower():
        return pd.DataFrame(self.scaler_X.inverse_transform(values), columns=self.input_columns)

    elif &#39;output&#39; in values_type.lower():
        return self.scaler_y.inverse_transform(values.reshape(-1, 1))

    else:
        raise ValueError(f&#34;values_type must be in [&#39;inputs&#39;, &#39;outputs&#39;], received {values_type}&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ise.data" href="index.html">ise.data</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ise.data.EmulatorData.EmulatorData" href="#ise.data.EmulatorData.EmulatorData">EmulatorData</a></code></h4>
<ul class="">
<li><code><a title="ise.data.EmulatorData.EmulatorData.create_boolean_indices" href="#ise.data.EmulatorData.EmulatorData.create_boolean_indices">create_boolean_indices</a></code></li>
<li><code><a title="ise.data.EmulatorData.EmulatorData.drop_columns" href="#ise.data.EmulatorData.EmulatorData.drop_columns">drop_columns</a></code></li>
<li><code><a title="ise.data.EmulatorData.EmulatorData.drop_missing" href="#ise.data.EmulatorData.EmulatorData.drop_missing">drop_missing</a></code></li>
<li><code><a title="ise.data.EmulatorData.EmulatorData.drop_outliers" href="#ise.data.EmulatorData.EmulatorData.drop_outliers">drop_outliers</a></code></li>
<li><code><a title="ise.data.EmulatorData.EmulatorData.process" href="#ise.data.EmulatorData.EmulatorData.process">process</a></code></li>
<li><code><a title="ise.data.EmulatorData.EmulatorData.scale" href="#ise.data.EmulatorData.EmulatorData.scale">scale</a></code></li>
<li><code><a title="ise.data.EmulatorData.EmulatorData.split_data" href="#ise.data.EmulatorData.EmulatorData.split_data">split_data</a></code></li>
<li><code><a title="ise.data.EmulatorData.EmulatorData.train_test_split" href="#ise.data.EmulatorData.EmulatorData.train_test_split">train_test_split</a></code></li>
<li><code><a title="ise.data.EmulatorData.EmulatorData.unscale" href="#ise.data.EmulatorData.EmulatorData.unscale">unscale</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>